import streamlit as st
import pandas as pd
import numpy as np
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import NearestNeighbors
from scipy import stats, interpolate, signal
from scipy.interpolate import interp1d
from scipy.stats import weibull_min, norm, gamma
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
from typing import List, Dict, Tuple, Optional, Any, Union
import io
import warnings
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import hashlib
from dataclasses import dataclass, field
warnings.filterwarnings('ignore')
# Suppress warnings
warnings.filterwarnings('ignore')


# Page configuration #ðŸ”§
st.set_page_config(
    page_title="Data Cleaning & Imputation Tool",
    page_icon="ðŸš§", 
    layout="wide",
    initial_sidebar_state="expanded"
)
if 'uploaded_file' not in st.session_state:
        st.session_state['uploaded_file'] = 'no'

uploaded_file = st.session_state['uploaded_file']

if(uploaded_file == 'yes'):
    if st.session_state.data_confirmed:

        df = st.session_state['df']  
        st.info('âšª Original dataset is loaded for this page.')
        if 'Date' not in df.columns:
            # Merge date and time into a timestamp column
            df['timestamp'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            # Drop Date and Time columns, reset index to timestamp    
            df = df.drop(columns=['Date', 'Time'])#.set_index('timestamp')
        st.sidebar.success(f"âœ… Loaded data with {len(df)} rows and {df.shape[1]} columns")

        ##### START #####
        st.markdown("""
        <style>
            .main {
                padding: 0rem 0rem;
            }
            .stProgress .st-bo {
                background-color: #667eea;
            }
            .stat-card {
                background-color: #f0f2f6;
                padding: 20px;
                border-radius: 10px;
                box-shadow: 0 2px 4px rgba(0,0,0,0.1);
                margin: 10px 0;
            }
            .stat-value {
                font-size: 2.5rem;
                font-weight: bold;
                color: #667eea;
            }
            .stat-label {
                font-size: 1rem;
                color: #666;
            }
            .guideline-card {
                background-color: #e8f4f8;
                padding: 15px;
                border-radius: 8px;
                border-left: 4px solid #3498db;
                margin: 10px 0;
            }
        </style>
        """, unsafe_allow_html=True)

        # Initialize session state
        if 'processed_data' not in st.session_state:
            st.session_state.processed_data = None
        if 'raw_data' not in st.session_state:
            st.session_state.raw_data = None
        if 'processing_log' not in st.session_state:
            st.session_state.processing_log = []
        if 'imputation_accuracy' not in st.session_state:
            st.session_state.imputation_accuracy = None
        if 'events_df' not in st.session_state:
            st.session_state.events_df = None
        if 'imputation_cache' not in st.session_state:
            st.session_state.imputation_cache = {}
        if 'temporal_patterns' not in st.session_state:
            st.session_state.temporal_patterns = {}

        # WHO and Auckland guidelines
        GUIDELINES = {
            'WHO': {
                'PM2.5': {'annual': 5, 'daily': 15, 'unit': 'Î¼g/mÂ³'},
                'PM10': {'annual': 15, 'daily': 45, 'unit': 'Î¼g/mÂ³'},
                'NO2': {'annual': 10, 'daily': 25, 'unit': 'Î¼g/mÂ³'},
                'AQI': {'good': (0, 50), 'moderate': (51, 100), 'unhealthy_sensitive': (101, 150), 
                        'unhealthy': (151, 200), 'very_unhealthy': (201, 300), 'hazardous': (301, 500)}
            },
            'Auckland': {
                'PM2.5': {'annual': 10, 'daily': 25, 'unit': 'Î¼g/mÂ³'},
                'PM10': {'annual': 20, 'daily': 50, 'unit': 'Î¼g/mÂ³'},
                'NO2': {'annual': 40, 'hourly': 200, 'unit': 'Î¼g/mÂ³'},
                'Temp': {'summer_avg': 20, 'winter_avg': 11, 'range': (-2, 30), 'unit': 'Â°C'},
                'WS': {'calm': (0, 0.5), 'light': (0.5, 3), 'moderate': (3, 8), 'strong': (8, 15), 'unit': 'm/s'}
            }
        }

        # Enhanced Chemical constraints from reference files
        CHEMICAL_CONSTRAINTS = {
            'nox_balance_tolerance': 0.00,  # Changed from 0.05 to 0 - NO TOLERANCE
            'no2_no_ratio_range': (0.1, 10.0),
            'no2_nox_ratio_typical': 0.7,
            'pm25_pm10_ratio_range': (0.3, 0.95),
            'pm_ratio_typical': 0.42,
            'max_values': {
                'NO': 800, 'NO2': 300, 'NOx': 1200,
                'PM2.5': 200, 'PM10': 300,
                'AQI': 500, 'WS': 50, 'Temp': 45
            },
            'rate_of_change_max': {
                'NO': 150, 'NO2': 100, 'NOx': 200,
                'PM2.5': 50, 'PM10': 75,
                'Temp': 5, 'WS': 10
            },
            'measurement_uncertainty': {
                'NO': 2.0, 'NO2': 3.0, 'NOx': 4.0,
                'PM2.5': 2.5, 'PM10': 5.0
            }
        }

        # Auckland-specific parameters
        AUCKLAND_LATITUDE = -36.8485
        AUCKLAND_LONGITUDE = 174.7633
        RUSH_HOURS = {
            'morning': [7, 8, 9],
            'evening': [17, 18, 19]
        }

        # Photochemical parameters for advanced NOx imputation
        PHOTOCHEMICAL_PARAMS = {
            'k1_o3_no': 1.8e-14,  # Rate constant for O3 + NO reaction
            'j_no2_peak': 0.008,  # Peak photolysis rate for NO2
            'solar_angle_threshold': 85,  # Degrees
            'voc_nox_threshold': 8.0,
            'o3_formation_threshold': 40
        }

        # Helper functions
        def log_message(message, type='info'):
            """Add message to processing log"""
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.processing_log.append({
                'time': timestamp,
                'message': message,
                'type': type
            })

        def check_event_dates(df):
            """Check if events_df is available and mark event dates"""
            if st.session_state.events_df is not None:
                log_message("Events data detected. Preserving event-related measurements.", 'info')
                
                # Convert dates to datetime for comparison
                df['Date_parsed'] = pd.to_datetime(df['Date'], errors='coerce')
                events_dates = pd.to_datetime(st.session_state.events_df['Date'], errors='coerce')
                
                # Mark rows that correspond to events
                df['has_event'] = df['Date_parsed'].isin(events_dates)
                
                event_count = df['has_event'].sum()
                log_message(f"Found {event_count} rows corresponding to events", 'info')
                
                return df
            else:
                df['has_event'] = False
                return df

        def count_missing_values(df, numeric_columns):
            """Accurately count missing values in numeric columns only"""
            missing_count = 0
            missing_details = {}
            
            for col in numeric_columns:
                if col in df.columns:
                    col_missing = df[col].isna().sum()
                    missing_count += col_missing
                    missing_details[col] = col_missing
            
            return missing_count, missing_details

        def apply_guidelines(value, column, guideline_type='Auckland'):
            """Apply WHO or Auckland guidelines to validate and adjust values"""
            if column in GUIDELINES[guideline_type]:
                guide = GUIDELINES[guideline_type][column]
                
                # For temperature, use seasonal ranges
                if column == 'Temp' and 'range' in guide:
                    min_val, max_val = guide['range']
                    return np.clip(value, min_val, max_val)
                
                # For AQI, ensure it's within valid range
                elif column == 'AQI' and guideline_type == 'WHO':
                    return np.clip(value, 0, 500)
                
                # For pollutants, check against daily limits
                elif 'daily' in guide:
                    # If value exceeds 3x daily limit, it might be an error
                    if value > guide['daily'] * 3:
                        return np.nan  # Mark for imputation
            
            return value

        def detect_anomalies(df, column):
            """Detect anomalies using multiple methods, excluding non-null event data from outlier detection"""
            anomalies = {
                'negative_values': [],
                'outliers': [],
                'impossible_values': [],
                'guideline_violations': []
            }
            
            # Check for negative values in columns that shouldn't have them
            non_negative_cols = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            if column in non_negative_cols:
                neg_mask = df[column] < 0
                anomalies['negative_values'] = df[neg_mask].index.tolist()
            
            # Enhanced outlier detection using MAD and IQR methods
            non_event_data = df[~df['has_event']][column].dropna()
            if len(non_event_data) > 10:
                # MAD (Median Absolute Deviation) method
                median = non_event_data.median()
                mad = np.median(np.abs(non_event_data - median))
                modified_z_score = 0.6745 * (df[column] - median) / (mad + 1e-6)
                
                # IQR method
                Q1 = non_event_data.quantile(0.25)
                Q3 = non_event_data.quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # Combine both methods for robust outlier detection
                outlier_mask = ((np.abs(modified_z_score) > 3.5) | 
                            ((df[column] < lower_bound) | (df[column] > upper_bound))) & \
                            ~(df['has_event'] & df[column].notna())
                anomalies['outliers'] = df[outlier_mask].index.tolist()
            
            # Domain-specific impossible values
            if column == 'WD':
                impossible_mask = (df[column] < 0) | (df[column] > 360)
                anomalies['impossible_values'] = df[impossible_mask].index.tolist()
            elif column == 'Temp':
                min_temp, max_temp = GUIDELINES['Auckland']['Temp']['range']
                impossible_mask = (df[column] < min_temp - 10) | (df[column] > max_temp + 10)
                anomalies['impossible_values'] = df[impossible_mask].index.tolist()
            
            # Check guideline violations
            if column in ['PM2.5', 'PM10', 'NO2']:
                for guideline_type in ['WHO', 'Auckland']:
                    if column in GUIDELINES[guideline_type] and 'daily' in GUIDELINES[guideline_type][column]:
                        limit = GUIDELINES[guideline_type][column]['daily']
                        violation_mask = (df[column] > limit * 3) & ~(df['has_event'] & df[column].notna())
                        anomalies['guideline_violations'].extend(df[violation_mask].index.tolist())
            
            return anomalies

        # Enhanced clean_data function to properly handle negative values
        def clean_data(df):
            """Clean data by fixing or removing anomalies while preserving non-null event data"""
            df_cleaned = df.copy()
            cleaning_stats = {}
            
            numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                            'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            # CRITICAL: Handle negative NO values first
            if 'NO' in df_cleaned.columns:
                neg_mask = df_cleaned['NO'] < 0
                neg_count = neg_mask.sum()
                if neg_count > 0:
                    log_message(f"Found {neg_count} negative NO values - setting to NaN for imputation", 'warning')
                    df_cleaned.loc[neg_mask, 'NO'] = np.nan
                    cleaning_stats['NO'] = {'cleaned': neg_count}
            
            for col in numeric_columns:
                if col in df_cleaned.columns and col != 'NO':  # NO already handled
                    cleaning_stats[col] = {'cleaned': 0}
                    
                    # Fix negative values for columns that shouldn't have them
                    if col in ['AQI', 'PM10', 'PM2.5', 'NO2', 'NOx', 'WS', 
                            'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                        neg_mask = df_cleaned[col] < 0
                        if neg_mask.any():
                            df_cleaned.loc[neg_mask, col] = np.nan
                            cleaning_stats[col]['cleaned'] += neg_mask.sum()
                            log_message(f"Set {neg_mask.sum()} negative {col} values to NaN", 'info')
                    
                    # Fix wind direction
                    if col == 'WD':
                        # Wrap around values
                        df_cleaned[col] = df_cleaned[col] % 360
                        # Fix negative values
                        neg_mask = df_cleaned[col] < 0
                        df_cleaned.loc[neg_mask, col] = df_cleaned.loc[neg_mask, col] + 360
                        cleaning_stats[col]['cleaned'] += neg_mask.sum()
                    
                    # Fix extreme temperatures using Auckland guidelines
                    if col == 'Temp':
                        min_temp, max_temp = GUIDELINES['Auckland']['Temp']['range']
                        extreme_mask = ((df_cleaned[col] < min_temp - 10) | (df_cleaned[col] > max_temp + 10))
                        cleaning_stats[col]['cleaned'] += extreme_mask.sum()
                        df_cleaned.loc[extreme_mask, col] = np.nan
            
            return df_cleaned, cleaning_stats

        # Enhanced helper functions for NOx and PM column detection
        def get_nox_columns(df):
            """Identify NOx family columns in the dataframe"""
            columns = {'no': None, 'no2': None, 'nox': None}
            
            for col in df.columns:
                col_upper = col.upper()
                if col_upper == 'NO' and columns['no'] is None:
                    columns['no'] = col
                elif col_upper == 'NO2' and columns['no2'] is None:
                    columns['no2'] = col
                elif col_upper in ['NOX', 'NO_X'] and columns['nox'] is None:
                    columns['nox'] = col
            
            return columns

        def get_pm_columns(df):
            """Identify PM family columns in the dataframe"""
            columns = {'pm25': None, 'pm10': None}
            
            for col in df.columns:
                if 'PM2.5' in col and columns['pm25'] is None:
                    columns['pm25'] = col
                elif 'PM10' in col and columns['pm10'] is None:
                    columns['pm10'] = col
            
            return columns

        # Advanced temporal feature extraction
        def add_temporal_features(df, latitude=AUCKLAND_LATITUDE):
            """Add comprehensive temporal features for imputation"""
            df_work = df.copy()
            
            # Ensure datetime index
            if 'DateTime' not in df_work.columns and 'Date' in df_work.columns and 'Time' in df_work.columns:
                df_work['DateTime'] = pd.to_datetime(df_work['Date'].astype(str) + ' ' + df_work['Time'].astype(str), errors='coerce')
            
            if 'DateTime' in df_work.columns:
                df_work['hour'] = df_work['DateTime'].dt.hour
                df_work['dayofweek'] = df_work['DateTime'].dt.dayofweek
                df_work['month'] = df_work['DateTime'].dt.month
                df_work['dayofyear'] = df_work['DateTime'].dt.dayofyear
                df_work['weekofyear'] = df_work['DateTime'].dt.isocalendar().week
                df_work['is_weekend'] = (df_work['dayofweek'] >= 5).astype(int)
                
                # Cyclical encoding
                df_work['hour_sin'] = np.sin(2 * np.pi * df_work['hour'] / 24)
                df_work['hour_cos'] = np.cos(2 * np.pi * df_work['hour'] / 24)
                df_work['month_sin'] = np.sin(2 * np.pi * df_work['month'] / 12)
                df_work['month_cos'] = np.cos(2 * np.pi * df_work['month'] / 12)
                df_work['dayofweek_sin'] = np.sin(2 * np.pi * df_work['dayofweek'] / 7)
                df_work['dayofweek_cos'] = np.cos(2 * np.pi * df_work['dayofweek'] / 7)
                
                # Season
                df_work['season'] = df_work['month'].apply(lambda x: 'Summer' if x in [12, 1, 2] else
                                                        'Winter' if x in [6, 7, 8] else
                                                        'Autumn' if x in [3, 4, 5] else 'Spring')
                
                # Rush hour indicators
                df_work['is_morning_rush'] = df_work['hour'].isin(RUSH_HOURS['morning']).astype(int)
                df_work['is_evening_rush'] = df_work['hour'].isin(RUSH_HOURS['evening']).astype(int)
                df_work['is_rush_hour'] = (df_work['is_morning_rush'] | df_work['is_evening_rush']).astype(int)
                
                # Solar angle approximation
                day_of_year = df_work['dayofyear']
                hour_angle = 15 * (df_work['hour'] - 12)
                declination = 23.45 * np.sin(np.radians(360 * (284 + day_of_year) / 365))
                
                lat_rad = np.radians(latitude)
                decl_rad = np.radians(declination)
                hour_rad = np.radians(hour_angle)
                
                df_work['solar_elevation'] = np.degrees(np.arcsin(
                    np.sin(lat_rad) * np.sin(decl_rad) + 
                    np.cos(lat_rad) * np.cos(decl_rad) * np.cos(hour_rad)
                ))
                
                # Daylight indicator
                df_work['is_daylight'] = (df_work['solar_elevation'] > 0).astype(int)
            
            return df_work

        # Advanced chemical balance imputation with proper variation
        def chemical_balance_imputation(df, column):
            """Apply chemical balance constraints for NOx and PM families with enhanced accuracy"""
            df_work = df.copy()
            imputed_count = 0
            
            # NOx family balance - CRITICAL FIX
            if column.upper() in ['NO', 'NO2', 'NOX']:
                nox_cols = get_nox_columns(df_work)
                
                if all(nox_cols.values()):
                    # Add temporal features for photochemical assessment
                    df_work = add_temporal_features(df_work)
                    
                    # CRITICAL: Fix negative values BEFORE any calculation
                    for nox_col in [nox_cols['no'], nox_cols['no2'], nox_cols['nox']]:
                        if nox_col in df_work.columns:
                            neg_mask = (df_work[nox_col] < 0) & ~df_work['has_event']
                            if neg_mask.any():
                                log_message(f"Setting {neg_mask.sum()} negative {nox_col} values to NaN for imputation", 'warning')
                                df_work.loc[neg_mask, nox_col] = np.nan
                    
                    # Case 1: Missing NOX, have NO and NO2 - MOST CRITICAL
                    if column.upper() == 'NOX':
                        mask = (df_work[column].isna() & 
                            df_work[nox_cols['no']].notna() & 
                            df_work[nox_cols['no2']].notna() & 
                            (df_work[nox_cols['no']] >= 0) &
                            (df_work[nox_cols['no2']] >= 0) &
                            ~df_work['has_event'])
                        
                        if mask.any():
                            # EXACT chemical balance - NO NOISE for NOx
                            df_work.loc[mask, column] = df_work.loc[mask, nox_cols['no']] + df_work.loc[mask, nox_cols['no2']]
                            imputed_count = mask.sum()
                            log_message(f"Chemical balance: Imputed {imputed_count} NOx values as EXACT NO + NO2", 'info')
                    
                    # Case 2: Missing NO, have NOX and NO2
                    elif column.upper() == 'NO':
                        mask = (df_work[column].isna() & 
                            df_work[nox_cols['nox']].notna() & 
                            df_work[nox_cols['no2']].notna() & 
                            (df_work[nox_cols['nox']] >= df_work[nox_cols['no2']]) &
                            (df_work[nox_cols['no2']] >= 0) &
                            ~df_work['has_event'])
                        
                        if mask.any():
                            # Calculate NO = NOx - NO2
                            df_work.loc[mask, column] = df_work.loc[mask, nox_cols['nox']] - df_work.loc[mask, nox_cols['no2']]
                            
                            # Ensure non-negative
                            df_work.loc[mask, column] = df_work.loc[mask, column].clip(lower=0)
                            
                            # CRITICAL: Verify balance immediately
                            balance_check = df_work.loc[mask, column] + df_work.loc[mask, nox_cols['no2']]
                            imbalance = np.abs(balance_check - df_work.loc[mask, nox_cols['nox']])
                            
                            # If imbalance > tolerance, recalculate without noise
                            if (imbalance > 4).any():
                                log_message("Enforcing exact NOx balance for NO calculation", 'info')
                                df_work.loc[mask, column] = (df_work.loc[mask, nox_cols['nox']] - 
                                                            df_work.loc[mask, nox_cols['no2']]).clip(lower=0)
                            
                            imputed_count = mask.sum()
                    
                    # Case 3: Missing NO2, have NOX and NO
                    elif column.upper() == 'NO2':
                        mask = (df_work[column].isna() & 
                            df_work[nox_cols['nox']].notna() & 
                            df_work[nox_cols['no']].notna() & 
                            (df_work[nox_cols['nox']] >= df_work[nox_cols['no']]) &
                            (df_work[nox_cols['no']] >= 0) &
                            ~df_work['has_event'])
                        
                        if mask.any():
                            # Calculate NO2 = NOx - NO
                            df_work.loc[mask, column] = df_work.loc[mask, nox_cols['nox']] - df_work.loc[mask, nox_cols['no']]
                            
                            # Ensure non-negative
                            df_work.loc[mask, column] = df_work.loc[mask, column].clip(lower=0)
                            
                            # CRITICAL: Verify balance immediately
                            balance_check = df_work.loc[mask, nox_cols['no']] + df_work.loc[mask, column]
                            imbalance = np.abs(balance_check - df_work.loc[mask, nox_cols['nox']])
                            
                            # If imbalance > tolerance, recalculate without noise
                            if (imbalance > 4).any():
                                log_message("Enforcing exact NOx balance for NO2 calculation", 'info')
                                df_work.loc[mask, column] = (df_work.loc[mask, nox_cols['nox']] - 
                                                            df_work.loc[mask, nox_cols['no']]).clip(lower=0)
                            
                            imputed_count = mask.sum()
                    
                    # CRITICAL NEW STEP: Post-imputation balance enforcement
                    if imputed_count > 0:
                        # Find all rows where we have complete NOx data
                        complete_mask = (df_work[nox_cols['no']].notna() & 
                                    df_work[nox_cols['no2']].notna() & 
                                    df_work[nox_cols['nox']].notna() &
                                    (df_work[nox_cols['no']] >= 0) &
                                    (df_work[nox_cols['no2']] >= 0))
                        
                        if complete_mask.any():
                            # ALWAYS recalculate NOx to ensure perfect balance
                            old_nox = df_work.loc[complete_mask, nox_cols['nox']].copy()
                            df_work.loc[complete_mask, nox_cols['nox']] = (
                                df_work.loc[complete_mask, nox_cols['no']] + 
                                df_work.loc[complete_mask, nox_cols['no2']]
                            )
                            
                            # Log changes
                            changes = (old_nox != df_work.loc[complete_mask, nox_cols['nox']]).sum()
                            if changes > 0:
                                log_message(f"Enforced exact NOx balance for {changes} rows", 'success')
            
            # PM family balance with Auckland-specific constraints
            elif 'PM' in column.upper():
                pm_cols = get_pm_columns(df_work)
                
                if all(pm_cols.values()):
                    # Auckland typical PM2.5/PM10 ratio
                    typical_ratio = 0.42
                    
                    # Impute PM2.5 from PM10
                    if 'PM2.5' in column:
                        mask = (df_work[column].isna() & 
                            df_work[pm_cols['pm10']].notna() & 
                            (df_work[pm_cols['pm10']] > 0) &
                            ~df_work['has_event'])
                        
                        if mask.any():
                            # Simple ratio-based imputation
                            df_work.loc[mask, column] = df_work.loc[mask, pm_cols['pm10']] * typical_ratio
                            
                            # Add small variation
                            noise = np.random.normal(0, 0.5, size=mask.sum())
                            df_work.loc[mask, column] = (df_work.loc[mask, column] + noise).clip(lower=0)
                            
                            # Ensure PM2.5 <= PM10
                            pm10_values = df_work.loc[mask, pm_cols['pm10']]
                            df_work.loc[mask, column] = df_work.loc[mask, column].clip(upper=pm10_values)
                            
                            imputed_count = mask.sum()
                    
                    # Impute PM10 from PM2.5
                    elif 'PM10' in column:
                        mask = (df_work[column].isna() & 
                            df_work[pm_cols['pm25']].notna() & 
                            (df_work[pm_cols['pm25']] > 0) &
                            ~df_work['has_event'])
                        
                        if mask.any():
                            df_work.loc[mask, column] = df_work.loc[mask, pm_cols['pm25']] / typical_ratio
                            
                            # Add small variation
                            noise = np.random.normal(0, 1.0, size=mask.sum())
                            df_work.loc[mask, column] = (df_work.loc[mask, column] + noise).clip(lower=0)
                            
                            # Ensure PM10 >= PM2.5
                            pm25_values = df_work.loc[mask, pm_cols['pm25']]
                            df_work.loc[mask, column] = df_work.loc[mask, column].clip(lower=pm25_values)
                            
                            imputed_count = mask.sum()
            
            if imputed_count > 0:
                log_message(f"Chemical balance imputation: {imputed_count} values imputed for {column}", 'info')
            
            # Clean up temporal features
            temp_features = ['hour', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 'is_weekend',
                            'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 
                            'dayofweek_cos', 'season', 'is_morning_rush', 'is_evening_rush', 
                            'is_rush_hour', 'solar_elevation', 'is_daylight']
            df_work = df_work.drop(columns=[col for col in temp_features if col in df_work.columns], errors='ignore')
            
            return df_work[column]

        # Enhanced pattern-based imputation with proper variation
        def pattern_based_imputation(df: pd.DataFrame, column: str) -> pd.DataFrame:
            """Impute using advanced pattern recognition with KNN and temporal patterns"""
            df_copy = df.copy()
            
            # Add temporal features
            df_copy = add_temporal_features(df_copy)
            
            # Use vectorized KNN approach for pattern matching
            feature_cols = ['hour', 'dayofweek', 'month', 'is_weekend', 'is_rush_hour']
            feature_cols = [col for col in feature_cols if col in df_copy.columns]
            
            if len(feature_cols) < 2:
                # Fallback to simple pattern-based
                return pattern_based_imputation_simple(df_copy, column)
            
            # Get valid and missing indices
            valid_mask = df_copy[column].notna() & ~df_copy['has_event']
            missing_mask = df_copy[column].isna()
            
            if not valid_mask.any() or not missing_mask.any():
                return df_copy[column]
            
            # Prepare features
            features = df_copy[feature_cols].copy()
            
            # Handle missing values in features
            for col in feature_cols:
                if features[col].isna().any():
                    features[col] = features[col].fillna(method='ffill').fillna(method='bfill')
                    if features[col].isna().any():
                        if col == 'hour':
                            features[col] = features[col].fillna(12)
                        elif col == 'dayofweek':
                            features[col] = features[col].fillna(3)
                        elif col == 'month':
                            features[col] = features[col].fillna(6)
                        else:
                            features[col] = features[col].fillna(0)
            
            valid_features = features[valid_mask].values
            missing_features = features[missing_mask].values
            
            # Scale features
            scaler = StandardScaler()
            valid_features_scaled = scaler.fit_transform(valid_features)
            missing_features_scaled = scaler.transform(missing_features)
            
            # Weight features based on importance
            feature_weights = np.array([2.0, 1.5, 1.0, 1.0, 1.5])[:len(feature_cols)]
            valid_features_weighted = valid_features_scaled * feature_weights
            missing_features_weighted = missing_features_scaled * feature_weights
            
            # Find nearest neighbors with adaptive k
            n_neighbors = min(50, max(10, len(valid_features) // 10))  # Adaptive k based on data size
            nn = NearestNeighbors(n_neighbors=n_neighbors, metric='manhattan', n_jobs=-1)
            nn.fit(valid_features_weighted)
            
            distances, indices = nn.kneighbors(missing_features_weighted)
            
            # Get values from nearest neighbors
            valid_values = df_copy.loc[valid_mask, column].values
            neighbor_values = valid_values[indices]
            
            # Calculate weights with distance decay and randomization
            # Use exponential decay with temperature parameter
            temperature = 1.0
            weights = np.exp(-distances / (temperature * distances.mean(axis=1, keepdims=True) + 1e-6))
            
            # Add stochastic element to weights
            weight_noise = np.random.uniform(0.8, 1.2, size=weights.shape)
            weights = weights * weight_noise
            
            # Normalize weights
            weights = weights / weights.sum(axis=1, keepdims=True)
            
            # Calculate imputed values with weighted sampling instead of just averaging
            imputed_values = []
            for i in range(len(missing_features)):
                # Use weighted random sampling from neighbors
                if np.random.random() < 0.7:  # 70% weighted average, 30% random sampling
                    value = np.sum(neighbor_values[i] * weights[i])
                else:
                    # Random sampling from neighbors based on weights
                    chosen_idx = np.random.choice(n_neighbors, p=weights[i])
                    value = neighbor_values[i, chosen_idx]
                
                imputed_values.append(value)
            
            imputed_values = np.array(imputed_values)
            
            # Apply imputed values
            missing_indices = df_copy[missing_mask].index
            df_copy.loc[missing_indices, column] = imputed_values
            
            # Add realistic temporal noise based on variable type and local variability
            neighbor_std = np.std(neighbor_values, axis=1)
            neighbor_std[neighbor_std == 0] = df_copy[column].std() * 0.05
            
            # Different noise levels for different variable types - REDUCED
            if column in ['NO', 'NO2', 'NOx']:
                noise_factor = 0.1  # Reduced from 0.2
            elif column in ['PM2.5', 'PM10']:
                noise_factor = 0.1  # Reduced from 0.15
            elif column == 'Temp':
                noise_factor = 0.05  # Reduced from 0.1
            elif column in ['Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                noise_factor = 0.15  # Reduced from 0.25
            else:
                noise_factor = 0.1  # Reduced from 0.2
            
            noise = np.random.normal(0, neighbor_std * noise_factor)
            df_copy.loc[missing_indices, column] += noise
            
            # Apply temporal smoothing with adaptive window
            window_size = 5  # Increased from 3
            for idx in missing_indices:
                idx_pos = df_copy.index.get_loc(idx)
                start = max(0, idx_pos - window_size)
                end = min(len(df_copy), idx_pos + window_size + 1)
                
                # Get local values
                local_values = df_copy.iloc[start:end][column]
                if local_values.notna().sum() > window_size // 2:  # Need at least half non-NaN
                    # Apply exponential weighted smoothing
                    weights = np.exp(-np.abs(np.arange(len(local_values)) - (idx_pos - start)) / 2)
                    weights[local_values.isna()] = 0
                    if weights.sum() > 0:
                        smoothed = np.sum(local_values.fillna(0) * weights) / weights.sum()
                        # Blend original and smoothed with more weight on original
                        df_copy.loc[idx, column] = 0.8 * df_copy.loc[idx, column] + 0.2 * smoothed
            
            # Ensure non-negative for appropriate columns
            non_negative_cols = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            if column in non_negative_cols:
                df_copy.loc[missing_indices, column] = df_copy.loc[missing_indices, column].clip(lower=0)
            
            # Apply physical constraints
            if column in CHEMICAL_CONSTRAINTS['max_values']:
                max_val = CHEMICAL_CONSTRAINTS['max_values'][column]
                df_copy.loc[missing_indices, column] = df_copy.loc[missing_indices, column].clip(upper=max_val)
            
            # Clean up and return only the column
            return df_copy[column]

        # Enhanced multivariate imputation with proper variation
        def multivariate_imputation(df: pd.DataFrame, column: str, related_columns: List[str]) -> pd.DataFrame:
            """Advanced multivariate imputation using ensemble of ML models with variation"""
            df_copy = df.copy()
            
            # Add temporal features
            df_copy = add_temporal_features(df_copy)
            
            # Enhance feature set
            all_features = related_columns.copy()
            temporal_features = ['hour', 'dayofweek', 'month', 'is_weekend', 'is_rush_hour',
                                'hour_sin', 'hour_cos', 'month_sin', 'month_cos']
            
            # Add lag features for time series
            for lag in [1, 6, 12, 24]:
                lag_col = f"{column}_lag_{lag}"
                df_copy[lag_col] = df_copy[column].shift(lag)
                all_features.append(lag_col)
            
            # Add rolling statistics with different windows
            for window in [6, 12, 24]:
                roll_mean = f"{column}_roll_mean_{window}"
                roll_std = f"{column}_roll_std_{window}"
                df_copy[roll_mean] = df_copy[column].rolling(window=window, min_periods=1).mean()
                df_copy[roll_std] = df_copy[column].rolling(window=window, min_periods=1).std()
                all_features.extend([roll_mean, roll_std])
            
            # Add temporal features to feature set
            all_features.extend([f for f in temporal_features if f in df_copy.columns])
            
            # Remove duplicates and ensure all features exist
            all_features = list(dict.fromkeys([f for f in all_features if f in df_copy.columns]))
            
            # Prepare training data
            train_mask = df_copy[column].notna() & ~df_copy['has_event']
            
            # Need sufficient training data
            if train_mask.sum() < 50 or len(all_features) < 3:
                log_message(f"Insufficient data for multivariate imputation of {column}", 'warning')
                return seasonal_imputation_with_guidelines(df, column)
            
            # Remove features with too many missing values
            feature_missing = df_copy[all_features].isna().sum()
            valid_features = [f for f in all_features if feature_missing[f] < len(df_copy) * 0.5]
            
            if len(valid_features) < 3:
                return seasonal_imputation_with_guidelines(df, column)
            
            # Prepare data
            X_train = df_copy.loc[train_mask, valid_features].values
            y_train = df_copy.loc[train_mask, column].values
            
            # Handle remaining NaN in features
            imputer = SimpleImputer(strategy='mean')
            X_train = imputer.fit_transform(X_train)
            
            # Scale features
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            
            # Train ensemble of models with different configurations
            models = {
                'rf1': RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42, n_jobs=-1),
                'rf2': RandomForestRegressor(n_estimators=50, max_depth=15, min_samples_split=10, random_state=123, n_jobs=-1),
                'rf3': RandomForestRegressor(n_estimators=75, max_depth=8, min_samples_split=8, random_state=456, n_jobs=-1),
                'gb': GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42),
                'ridge': Ridge(alpha=1.0)
            }
            
            trained_models = {}
            model_scores = {}
            
            for name, model in models.items():
                try:
                    # Cross-validation score
                    scores = cross_val_score(model, X_train_scaled, y_train, 
                                        cv=min(5, train_mask.sum()//10), scoring='r2')
                    model_scores[name] = scores.mean()
                    
                    # Train model
                    model.fit(X_train_scaled, y_train)
                    trained_models[name] = model
                except:
                    continue
            
            if not trained_models:
                return seasonal_imputation_with_guidelines(df, column)
            
            # Predict missing values
            missing_mask = df_copy[column].isna()
            if missing_mask.sum() > 0:
                X_missing = df_copy.loc[missing_mask, valid_features].values
                X_missing = imputer.transform(X_missing)
                X_missing_scaled = scaler.transform(X_missing)
                
                # Get predictions from each model
                predictions = []
                weights = []
                uncertainties = []
                
                for name, model in trained_models.items():
                    pred = model.predict(X_missing_scaled)
                    
                    # Calculate model-specific uncertainty
                    if hasattr(model, 'estimators_') and name.startswith('rf'):  # Random Forest
                        # Get predictions from each tree - Fixed approach
                        try:
                            # For sklearn RandomForest, use predict method on each estimator
                            tree_preds = []
                            for estimator in model.estimators_:
                                tree_pred = estimator.predict(X_missing_scaled)
                                tree_preds.append(tree_pred)
                            tree_preds = np.array(tree_preds)
                            uncertainty = np.std(tree_preds, axis=0)
                        except:
                            # Fallback: use residual-based uncertainty
                            train_pred = model.predict(X_train_scaled)
                            residual_std = np.std(y_train - train_pred)
                            uncertainty = np.full(len(pred), residual_std)
                    elif hasattr(model, 'estimators_'):  # Gradient Boosting
                        # For gradient boosting, use staged predictions
                        try:
                            staged_preds = list(model.staged_predict(X_missing_scaled))
                            if len(staged_preds) > 10:
                                # Use last 10 stages for uncertainty
                                last_stages = np.array(staged_preds[-10:])
                                uncertainty = np.std(last_stages, axis=0)
                            else:
                                # Fallback to residual-based
                                train_pred = model.predict(X_train_scaled)
                                residual_std = np.std(y_train - train_pred)
                                uncertainty = np.full(len(pred), residual_std)
                        except:
                            # Fallback to residual-based
                            train_pred = model.predict(X_train_scaled)
                            residual_std = np.std(y_train - train_pred)
                            uncertainty = np.full(len(pred), residual_std)
                    else:
                        # For other models, use residual-based uncertainty
                        train_pred = model.predict(X_train_scaled)
                        residual_std = np.std(y_train - train_pred)
                        uncertainty = np.full(len(pred), residual_std)
                    
                    predictions.append(pred)
                    weights.append(max(0, model_scores.get(name, 0)))
                    uncertainties.append(uncertainty)
                
                if sum(weights) > 0:
                    # Weighted average of predictions
                    weights = np.array(weights) / sum(weights)
                    
                    # Use stochastic weighted combination
                    ensemble_pred = np.zeros(len(X_missing))
                    ensemble_uncertainty = np.zeros(len(X_missing))
                    
                    for i in range(len(X_missing)):
                        # Randomly select model based on weights for each prediction
                        if np.random.random() < 0.8:  # 80% weighted average
                            ensemble_pred[i] = np.sum([p[i] * w for p, w in zip(predictions, weights)])
                            ensemble_uncertainty[i] = np.sqrt(np.sum([u[i]**2 * w for u, w in zip(uncertainties, weights)]))
                        else:  # 20% random model selection
                            chosen_model = np.random.choice(len(predictions), p=weights)
                            ensemble_pred[i] = predictions[chosen_model][i]
                            ensemble_uncertainty[i] = uncertainties[chosen_model][i]
                    
                    # Add prediction uncertainty as noise
                    noise = np.random.normal(0, ensemble_uncertainty * 0.3)
                    ensemble_pred += noise
                    
                    # Apply predictions
                    df_copy.loc[missing_mask, column] = ensemble_pred
                    
                    # Apply constraints based on column type
                    if column in CHEMICAL_CONSTRAINTS['max_values']:
                        df_copy.loc[missing_mask, column] = df_copy.loc[missing_mask, column].clip(
                            lower=0, upper=CHEMICAL_CONSTRAINTS['max_values'][column]
                        )
            
            # Clean up temporary features
            temp_cols = [col for col in df_copy.columns if any(pattern in col for pattern in 
                        ['_lag_', '_roll_', 'hour', 'dayofweek', 'month', 'is_weekend', 
                        'is_rush_hour', '_sin', '_cos', 'season', 'solar_elevation', 'is_daylight'])]
            
            # Return only the imputed column
            return df_copy[column]

        # Enhanced seasonal imputation with realistic variation
        def seasonal_imputation_with_guidelines(df: pd.DataFrame, column: str) -> pd.DataFrame:
            """Impute using seasonal patterns with guideline awareness and physical models"""
            df_copy = df.copy()
            
            # Add temporal features
            df_copy = add_temporal_features(df_copy)
            
            # Extract hour from Time column if not already present
            if 'Hour' not in df_copy.columns and 'Time' in df_copy.columns:
                df_copy['Hour'] = pd.to_datetime(df_copy['Time'], format='%H:%M:%S', errors='coerce').dt.hour
            
            # For temperature, use advanced physical model
            if column == 'Temp':
                # Calculate seasonal component
                if 'dayofyear' in df_copy.columns:
                    seasonal_phase = 30  # Days after Jan 1 for summer peak in Southern Hemisphere
                    seasonal_amplitude = 7  # Temperature amplitude
                    base_temp = df_copy[~df_copy['has_event']][column].mean() if df_copy[~df_copy['has_event']][column].notna().any() else 15
                    
                    seasonal_component = base_temp + seasonal_amplitude * np.sin(
                        2 * np.pi * (df_copy['dayofyear'] - seasonal_phase) / 365.25
                    )
                    
                    # Add realistic weather variation
                    daily_variation = np.random.normal(0, 2.5, size=len(df_copy))
                    seasonal_component += daily_variation
                    
                    # Calculate diurnal patterns by season
                    seasonal_hourly_stats = df_copy[~df_copy['has_event']].groupby(['season', 'hour'])[column].agg(['mean', 'std', 'count'])
                    
                    # Fill missing values
                    missing_mask = df_copy[column].isna()
                    for idx in df_copy[missing_mask].index:
                        loc = df_copy.index.get_loc(idx)
                        season = df_copy.loc[idx, 'season'] if 'season' in df_copy.columns else 'Unknown'
                        hour = df_copy.loc[idx, 'hour'] if 'hour' in df_copy.columns else 12
                        
                        if (season, hour) in seasonal_hourly_stats.index and seasonal_hourly_stats.loc[(season, hour), 'count'] > 5:
                            # Use seasonal-hourly statistics
                            diurnal_mean = seasonal_hourly_stats.loc[(season, hour), 'mean']
                            diurnal_std = seasonal_hourly_stats.loc[(season, hour), 'std']
                            
                            # Blend seasonal and diurnal components
                            seasonal_value = seasonal_component.iloc[loc] if loc < len(seasonal_component) else base_temp
                            weight = min(seasonal_hourly_stats.loc[(season, hour), 'count'] / 100, 0.8)
                            
                            base_value = weight * diurnal_mean + (1 - weight) * seasonal_value
                            
                            # Add realistic variation
                            if pd.notna(diurnal_std) and diurnal_std > 0:
                                variation = np.random.normal(0, diurnal_std * 0.7)
                            else:
                                variation = np.random.normal(0, 2.0)
                            
                            df_copy.loc[idx, column] = base_value + variation
                        else:
                            # Fallback to seasonal average
                            df_copy.loc[idx, column] = seasonal_component.iloc[loc] if loc < len(seasonal_component) else base_temp
                            df_copy.loc[idx, column] += np.random.normal(0, 2.5)
            
            # For other variables
            else:
                # Calculate comprehensive statistics
                hourly_stats = df_copy[~df_copy['has_event']].groupby('hour')[column].agg(['mean', 'std', 'count', 'quantile'])
                
                # Also calculate day-of-week patterns
                dow_hourly_stats = df_copy[~df_copy['has_event']].groupby(['dayofweek', 'hour'])[column].agg(['mean', 'std', 'count'])
                
                missing_mask = df_copy[column].isna()
                for idx in df_copy[missing_mask].index:
                    hour = df_copy.loc[idx, 'hour'] if 'hour' in df_copy.columns else 12
                    dow = df_copy.loc[idx, 'dayofweek'] if 'dayofweek' in df_copy.columns else 3
                    
                    # Try day-of-week specific pattern first
                    if (dow, hour) in dow_hourly_stats.index and dow_hourly_stats.loc[(dow, hour), 'count'] > 10:
                        mean_val = dow_hourly_stats.loc[(dow, hour), 'mean']
                        std_val = dow_hourly_stats.loc[(dow, hour), 'std']
                        
                        if pd.notna(mean_val):
                            # Add variation based on local statistics
                            if pd.notna(std_val) and std_val > 0:
                                # Use beta distribution for bounded variation
                                if column in ['WS', 'WD', 'AQI']:
                                    # For bounded variables, use beta distribution
                                    alpha = 2
                                    beta = 2
                                    variation = (np.random.beta(alpha, beta) - 0.5) * 2 * std_val * 0.6
                                else:
                                    variation = np.random.normal(0, std_val * 0.6)
                            else:
                                col_std = df_copy[column].std()
                                variation = np.random.normal(0, col_std * 0.4) if pd.notna(col_std) else 0
                            
                            df_copy.loc[idx, column] = mean_val + variation
                    
                    # Fallback to hourly pattern
                    elif hour in hourly_stats.index and hourly_stats.loc[hour, 'count'] > 5:
                        mean_val = hourly_stats.loc[hour, 'mean']
                        std_val = hourly_stats.loc[hour, 'std']
                        
                        if pd.notna(mean_val):
                            if pd.notna(std_val) and std_val > 0:
                                variation = np.random.normal(0, std_val * 0.6)
                            else:
                                col_std = df_copy[column].std()
                                variation = np.random.normal(0, col_std * 0.4) if pd.notna(col_std) else 0
                            
                            df_copy.loc[idx, column] = mean_val + variation
                    
                    # Final fallback to overall statistics
                    else:
                        overall_stats = df_copy[~df_copy['has_event']][column].describe()
                        if pd.notna(overall_stats['mean']):
                            # Use truncated normal for realistic values
                            mean_val = overall_stats['mean']
                            std_val = overall_stats['std']
                            
                            if pd.notna(std_val) and std_val > 0:
                                # Sample from truncated normal
                                lower = overall_stats['25%']
                                upper = overall_stats['75%']
                                
                                # Generate value within reasonable range
                                value = np.random.normal(mean_val, std_val * 0.5)
                                value = np.clip(value, lower - std_val, upper + std_val)
                                
                                df_copy.loc[idx, column] = value
                            else:
                                df_copy.loc[idx, column] = mean_val
            
            # Clean up temporal features and return only the column
            return df_copy[column]

        def pattern_based_imputation_simple(df: pd.DataFrame, column: str) -> pd.DataFrame:
            """Simple pattern-based imputation fallback with variation"""
            df_copy = df.copy()
            
            # Parse datetime
            if 'DateTime' not in df_copy.columns:
                df_copy['DateTime'] = pd.to_datetime(df_copy['Date'].astype(str) + ' ' + df_copy['Time'].astype(str), errors='coerce')
            
            df_copy['DayOfWeek'] = df_copy['DateTime'].dt.dayofweek
            df_copy['Hour'] = df_copy['DateTime'].dt.hour
            
            # Calculate patterns excluding all event days
            pattern_stats = df_copy[~df_copy['has_event']].groupby(['DayOfWeek', 'Hour'])[column].agg(['mean', 'std', 'count'])
            
            # Fill missing values only
            missing_mask = df_copy[column].isna()
            for idx in df_copy[missing_mask].index:
                dow = df_copy.loc[idx, 'DayOfWeek']
                hour = df_copy.loc[idx, 'Hour']
                
                if (dow, hour) in pattern_stats.index and pattern_stats.loc[(dow, hour), 'count'] > 5:
                    mean_val = pattern_stats.loc[(dow, hour), 'mean']
                    std_val = pattern_stats.loc[(dow, hour), 'std']
                    
                    if pd.notna(mean_val):
                        # Add realistic variation
                        if pd.notna(std_val) and std_val > 0:
                            variation = np.random.normal(0, std_val * 0.5)
                        else:
                            # Use 10% of mean as variation
                            variation = np.random.normal(0, abs(mean_val) * 0.1)
                        
                        df_copy.loc[idx, column] = max(0, mean_val + variation)
                else:
                    # Fallback chain with variation
                    hour_stats = df_copy[~df_copy['has_event']].groupby('Hour')[column].agg(['mean', 'std'])
                    if hour in hour_stats.index and pd.notna(hour_stats.loc[hour, 'mean']):
                        mean_val = hour_stats.loc[hour, 'mean']
                        std_val = hour_stats.loc[hour, 'std']
                        variation = np.random.normal(0, std_val * 0.5) if pd.notna(std_val) else np.random.normal(0, mean_val * 0.1)
                        df_copy.loc[idx, column] = max(0, mean_val + variation)
                    else:
                        overall_mean = df_copy[~df_copy['has_event']][column].mean()
                        overall_std = df_copy[~df_copy['has_event']][column].std()
                        if pd.notna(overall_mean):
                            variation = np.random.normal(0, overall_std * 0.5) if pd.notna(overall_std) else 0
                            df_copy.loc[idx, column] = max(0, overall_mean + variation)
            
            return df_copy[column]

        # Photochemical imputation for NOx species
        def photochemical_imputation(df: pd.DataFrame, column: str) -> pd.DataFrame:
            """Apply photochemical relationships for NOx imputation with variation"""
            df_work = df.copy()
            
            if column.upper() not in ['NO', 'NO2', 'NOX'] or 'O3' not in df_work.columns:
                return df_work[column]
            
            # Add temporal features including solar angle
            df_work = add_temporal_features(df_work)
            
            nox_cols = get_nox_columns(df_work)
            missing_mask = df_work[column].isna()
            
            if not missing_mask.any() or 'solar_elevation' not in df_work.columns:
                return df_work[column]
            
            # Photochemical relationships during daylight
            daylight_mask = df_work['solar_elevation'] > 0
            
            if column.upper() == 'NO2' and nox_cols['no'] and df_work[nox_cols['no']].notna().any():
                # Estimate NO2 from NO and O3 using photostationary state
                mask = missing_mask & daylight_mask & df_work[nox_cols['no']].notna() & df_work['O3'].notna()
                
                if mask.any():
                    # Photolysis rate depends on solar angle with variation
                    base_j_no2 = PHOTOCHEMICAL_PARAMS['j_no2_peak'] * np.sin(np.radians(df_work.loc[mask, 'solar_elevation']))
                    # Add cloud cover variation (Â±30%)
                    j_no2 = base_j_no2 * (1 + 0.3 * (np.random.randn(mask.sum()) - 0.5))
                    j_no2 = np.maximum(j_no2, 1e-10)
                    
                    k1 = PHOTOCHEMICAL_PARAMS['k1_o3_no']
                    
                    # Photostationary state with variation
                    ratio = (k1 * df_work.loc[mask, 'O3']) / j_no2
                    
                    # Add measurement uncertainty
                    ratio_variation = ratio * (1 + 0.1 * np.random.randn(mask.sum()))
                    
                    df_work.loc[mask, column] = df_work.loc[mask, nox_cols['no']] * ratio_variation
                    
                    # Add measurement noise
                    noise = np.random.normal(0, CHEMICAL_CONSTRAINTS['measurement_uncertainty']['NO2'], size=mask.sum())
                    df_work.loc[mask, column] += noise
                    
                    # Apply reasonable bounds
                    df_work.loc[mask, column] = df_work.loc[mask, column].clip(
                        lower=0, upper=CHEMICAL_CONSTRAINTS['max_values']['NO2']
                    )
            
            elif column.upper() == 'NO' and nox_cols['no2'] and df_work[nox_cols['no2']].notna().any():
                # Reverse calculation
                mask = missing_mask & daylight_mask & df_work[nox_cols['no2']].notna() & df_work['O3'].notna()
                
                if mask.any():
                    base_j_no2 = PHOTOCHEMICAL_PARAMS['j_no2_peak'] * np.sin(np.radians(df_work.loc[mask, 'solar_elevation']))
                    j_no2 = base_j_no2 * (1 + 0.3 * (np.random.randn(mask.sum()) - 0.5))
                    j_no2 = np.maximum(j_no2, 1e-10)
                
                    k1 = PHOTOCHEMICAL_PARAMS['k1_o3_no']
                    
                    # [NO] = J(NO2)[NO2]/(k1[O3])
                    base_no = (j_no2 * df_work.loc[mask, nox_cols['no2']]) / (k1 * df_work.loc[mask, 'O3'] + 1e-10)
                    
                    # Add variation for atmospheric variability
                    variation_factor = 1 + 0.15 * np.random.randn(mask.sum())
                    df_work.loc[mask, column] = base_no * variation_factor
                    
                    # Add measurement noise
                    noise = np.random.normal(0, CHEMICAL_CONSTRAINTS['measurement_uncertainty']['NO'], size=mask.sum())
                    df_work.loc[mask, column] += noise
                    
                    df_work.loc[mask, column] = df_work.loc[mask, column].clip(
                        lower=0, upper=CHEMICAL_CONSTRAINTS['max_values']['NO']
                    )
            
            # Clean up temporal features and return only the column
            return df_work[column]

        # Enhanced fallback imputation with multiple strategies
        def apply_fallback(df: pd.DataFrame) -> Tuple[pd.DataFrame, Dict]:
            """Applies intelligent fallback imputation to ensure no NaNs remain"""
            imputed_df = df.copy()
            report = {'total_filled_count': 0, 'by_variable': {}}
            
            # DEBUG: Check what columns are being processed
            all_columns = list(imputed_df.columns)
            log_message(f"Total columns in dataframe: {len(all_columns)}", 'info')
            
            # ADD THIS BLOCK HERE - Check for temporal columns that shouldn't be processed
            temp_features = ['hour', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 'is_weekend',
                            'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 
                            'dayofweek_cos', 'season', 'is_morning_rush', 'is_evening_rush', 
                            'is_rush_hour', 'solar_elevation', 'is_daylight', 'DateTime']
            
            temporal_cols_present = [col for col in all_columns if col in temp_features]
            if temporal_cols_present:
                log_message(f"Found {len(temporal_cols_present)} temporal columns to exclude: {temporal_cols_present[:5]}...", 'info')
                # Show which columns are data vs temporal
                data_cols_count = len([col for col in all_columns if col not in temp_features])
                log_message(f"Data columns: {data_cols_count}, Temporal columns: {len(temporal_cols_present)}", 'info')
                
            # CRITICAL FIX: Only process truly numeric columns that need imputation
            numeric_cols = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                            'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            # Skip temporal features that might have been added
            cols_to_process = [col for col in numeric_cols if col in imputed_df.columns]
            
            # Count actual missing values
            total_missing = 0
            for col in cols_to_process:
                col_missing = imputed_df[col].isnull().sum()
                total_missing += col_missing
                if col_missing > 0:
                    log_message(f"Fallback needed for {col}: {col_missing} missing values", 'info')
            
            if total_missing == 0:
                log_message("No missing values found - skipping fallback", 'info')
                return imputed_df, report
            
            log_message(f"Applying fallback for {total_missing} missing values", 'info')
            
            if total_missing > 0:
                log_message("Applying intelligent fallback imputation for remaining NaNs.", 'warning')
                
                # Add temporal features for advanced interpolation
                imputed_df = add_temporal_features(imputed_df)
                
                # Process only the specific numeric columns
                for col in cols_to_process:
                    if imputed_df[col].isnull().any():
                        missing_before = imputed_df[col].isnull().sum()
                        
                        # Skip NOx family if they should be complete from chemical balance
                        if col in ['NO', 'NO2', 'NOx']:
                            # Check if this is a chemical balance issue
                            nox_cols = get_nox_columns(imputed_df)
                            mask = imputed_df[col].isna()
                            
                            # Try chemical balance one more time
                            if mask.any():
                                log_message(f"Attempting final chemical balance for {col}", 'warning')
                                imputed_df[col] = chemical_balance_imputation(imputed_df, col)
                        
                        # Try different interpolation methods in order of preference
                        methods_tried = []
                        
                        # 1. Kalman filter for smooth time series
                        if imputed_df[col].isnull().any():
                            try:
                                from pykalman import KalmanFilter
                                kf = KalmanFilter(initial_state_mean=imputed_df[col].mean(), n_dim_obs=1)
                                imputed_values = kf.smooth(imputed_df[col].values)[0]
                                imputed_df.loc[imputed_df[col].isna(), col] = imputed_values[imputed_df[col].isna()]
                                methods_tried.append('kalman')
                            except:
                                pass
                        
                        # 2. Spline interpolation for smooth curves
                        if imputed_df[col].isnull().any():
                            try:
                                valid_indices = imputed_df[col].notna()
                                if valid_indices.sum() > 3:
                                    imputed_df[col] = imputed_df[col].interpolate(method='spline', order=3, limit_direction='both')
                                    
                                    # FIX: Check constraints immediately
                                    if col in ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                            'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                                        mask_negative = imputed_df[col] < 0
                                        if mask_negative.any():
                                            log_message(f"Spline created {mask_negative.sum()} negative values in {col}, reverting", 'warning')
                                            imputed_df.loc[mask_negative, col] = np.nan
                                    
                                    methods_tried.append('spline')
                            except:
                                pass
                        
                        # 3. Time-based interpolation
                        if imputed_df[col].isnull().any() and 'DateTime' in imputed_df.columns:
                            try:
                                temp_df = imputed_df.set_index('DateTime')
                                temp_df[col] = temp_df[col].interpolate(method='time', limit_direction='both')
                                imputed_df[col] = temp_df[col].values
                                
                                # Check constraints
                                if col in ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                        'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                                    mask_negative = imputed_df[col] < 0
                                    if mask_negative.any():
                                        imputed_df.loc[mask_negative, col] = np.nan
                                
                                methods_tried.append('time')
                            except:
                                pass
                        
                        # 4. Linear interpolation with extended limit
                        if imputed_df[col].isnull().any():
                            try:
                                imputed_df[col] = imputed_df[col].interpolate(method='linear', limit_direction='both', limit=48)
                                
                                # Check constraints
                                if col in ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                        'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                                    mask_negative = imputed_df[col] < 0
                                    if mask_negative.any():
                                        imputed_df.loc[mask_negative, col] = np.nan
                                
                                methods_tried.append('linear')
                            except:
                                pass
                        
                        # 5. Forward/backward fill
                        if imputed_df[col].isnull().any():
                            try:
                                imputed_df[col] = imputed_df[col].ffill(limit=24).bfill(limit=24)
                                methods_tried.append('ffill/bfill')
                            except:
                                pass
                        
                        # 6. Seasonal average with variation
                        if imputed_df[col].isnull().any() and 'month' in imputed_df.columns:
                            monthly_stats = imputed_df.groupby('month')[col].agg(['mean', 'std'])
                            mask = imputed_df[col].isna()
                            for idx in imputed_df[mask].index:
                                month = imputed_df.loc[idx, 'month']
                                if month in monthly_stats.index and pd.notna(monthly_stats.loc[month, 'mean']):
                                    mean_val = monthly_stats.loc[month, 'mean']
                                    std_val = monthly_stats.loc[month, 'std']
                                    # Add variation
                                    if pd.notna(std_val) and std_val > 0:
                                        value = np.random.normal(mean_val, std_val * 0.3)
                                    else:
                                        value = mean_val
                                    
                                    # Ensure non-negative for appropriate columns
                                    if col in ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                            'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                                        value = max(0.1, value)
                                    
                                    imputed_df.loc[idx, col] = value
                            methods_tried.append('seasonal_avg')
                        
                        # 7. Final resort: global mean with noise
                        if imputed_df[col].isnull().any():
                            col_mean = imputed_df[col].mean()
                            col_std = imputed_df[col].std()
                            
                            if pd.notna(col_mean):
                                missing_count = imputed_df[col].isnull().sum()
                                
                                # FIX: Smart fallback based on column type
                                if col in ['NO', 'NO2', 'NOx']:
                                    # For NOx family, use typical Auckland values
                                    typical_values = {
                                        'NO': 15.0,   # Typical Auckland NO
                                        'NO2': 20.0,  # Typical Auckland NO2  
                                        'NOx': 35.0   # Should be NO + NO2
                                    }
                                    base_value = typical_values.get(col, col_mean)
                                    
                                    # Add small variation
                                    if pd.notna(col_std) and col_std > 0:
                                        noise = np.random.normal(0, col_std * 0.1, size=missing_count)
                                        values = base_value + noise
                                    else:
                                        values = np.full(missing_count, base_value)
                                    
                                    # Ensure non-negative
                                    values = np.maximum(values, 0.1)
                                    imputed_df.loc[imputed_df[col].isna(), col] = values
                                    
                                elif col in ['PM10', 'PM2.5']:
                                    # For PM, use ratio-based approach
                                    if col == 'PM2.5' and 'PM10' in imputed_df.columns:
                                        pm10_values = imputed_df.loc[imputed_df[col].isna(), 'PM10']
                                        valid_pm10 = pm10_values.notna()
                                        if valid_pm10.any():
                                            # Use Auckland typical ratio
                                            imputed_df.loc[imputed_df[col].isna() & valid_pm10, col] = pm10_values[valid_pm10] * 0.42
                                    
                                    # Fill remaining with mean
                                    remaining_missing = imputed_df[col].isna()
                                    if remaining_missing.any():
                                        if pd.notna(col_std) and col_std > 0:
                                            noise = np.random.normal(0, col_std * 0.1, size=remaining_missing.sum())
                                            values = col_mean + noise
                                            values = np.maximum(values, 0.1)
                                            imputed_df.loc[remaining_missing, col] = values
                                        else:
                                            imputed_df.loc[remaining_missing, col] = max(0.1, col_mean)
                                
                                else:
                                    # Standard approach for other columns
                                    if pd.notna(col_std) and col_std > 0:
                                        noise = np.random.normal(0, col_std * 0.2, size=missing_count)
                                        values = col_mean + noise
                                        
                                        # Ensure non-negative for appropriate columns
                                        if col in ['AQI', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']:
                                            values = np.maximum(values, 0.1)
                                        
                                        imputed_df.loc[imputed_df[col].isna(), col] = values
                                    else:
                                        imputed_df.loc[imputed_df[col].isna(), col] = col_mean
                            else:
                                # If column mean is NaN, use appropriate defaults
                                default_values = {
                                    'NO': 15.0, 'NO2': 20.0, 'NOx': 35.0,
                                    'PM10': 20.0, 'PM2.5': 8.4,
                                    'Temp': 15.0, 'WS': 3.0, 'WD': 180.0,
                                    'AQI': 50.0,
                                    'Total_Pedestrians': 1000.0,
                                    'City_Centre_TVCount': 5000.0,
                                    'TrafficV': 500.0
                                }
                                log_message(f"Using default value for {col}", 'warning')
                                imputed_df[col] = imputed_df[col].fillna(default_values.get(col, 0))
                            
                            methods_tried.append('mean/default')
                        
                        filled_count = missing_before - imputed_df[col].isnull().sum()
                        if filled_count > 0:
                            report['by_variable'][col] = {
                                'values_imputed': filled_count, 
                                'methods': methods_tried,
                                'final_method': methods_tried[-1] if methods_tried else 'none'
                            }
                            report['total_filled_count'] += filled_count
                
                # CRITICAL: Re-enforce NOx balance after all fallback
                nox_cols = get_nox_columns(imputed_df)
                if all(nox_cols.values()):
                    mask = (imputed_df[nox_cols['no']].notna() & 
                            imputed_df[nox_cols['no2']].notna())
                    
                    if mask.any():
                        # Fix any negative values first
                        no_neg = imputed_df.loc[mask, nox_cols['no']] < 0
                        no2_neg = imputed_df.loc[mask, nox_cols['no2']] < 0
                        
                        if no_neg.any():
                            imputed_df.loc[mask & no_neg, nox_cols['no']] = 0.1
                            log_message(f"Fixed {no_neg.sum()} negative NO values in fallback", 'warning')
                        
                        if no2_neg.any():
                            imputed_df.loc[mask & no2_neg, nox_cols['no2']] = 0.1
                            log_message(f"Fixed {no2_neg.sum()} negative NO2 values in fallback", 'warning')
                        
                        # Recalculate NOx to ensure balance
                        old_nox = imputed_df.loc[mask, nox_cols['nox']].copy()
                        imputed_df.loc[mask, nox_cols['nox']] = (
                            imputed_df.loc[mask, nox_cols['no']] + 
                            imputed_df.loc[mask, nox_cols['no2']]
                        )
                        
                        changed = (old_nox != imputed_df.loc[mask, nox_cols['nox']]).sum()
                        if changed > 0:
                            log_message(f"Re-enforced NOx balance for {changed} rows after fallback", 'info')
                            report['nox_balance_fixed'] = changed
                
                # Clean up temporal features
                temp_features = ['hour', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 'is_weekend',
                                'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'dayofweek_sin', 
                                'dayofweek_cos', 'season', 'is_morning_rush', 'is_evening_rush', 
                                'is_rush_hour', 'solar_elevation', 'is_daylight']
                
                for col in temp_features:
                    if col in imputed_df.columns and col not in df.columns:
                        imputed_df = imputed_df.drop(columns=[col])
                
                if report['total_filled_count'] > 0:
                    log_message(f"Applied intelligent fallback, imputing {report['total_filled_count']} remaining values.", 'info')
            
            return imputed_df, report

        # Enhanced enforce_constraints with ITERATIVE checking

        def enforce_constraints(df):
            """Enforce all chemical and physical constraints after imputation - ENHANCED"""
            df_work = df.copy()
            
            max_iterations = 10
            tolerance_nox = 0.1  # Tighter tolerance for NOx balance
            
            for iteration in range(max_iterations):
                violations_found = False
                violation_count = 0
                
                # 1. NOx balance constraint - MOST CRITICAL
                nox_cols = get_nox_columns(df_work)
                if all(nox_cols.values()):
                    mask = (df_work[nox_cols['no']].notna() & 
                            df_work[nox_cols['no2']].notna() & 
                            df_work[nox_cols['nox']].notna())
                    
                    if mask.any():
                        # ALWAYS recalculate NOx = NO + NO2 with NO TOLERANCE
                        old_nox = df_work.loc[mask, nox_cols['nox']].copy()
                        df_work.loc[mask, nox_cols['nox']] = (
                            df_work.loc[mask, nox_cols['no']] + 
                            df_work.loc[mask, nox_cols['no2']]
                        )
                        
                        # Check if any values changed
                        changed = (old_nox != df_work.loc[mask, nox_cols['nox']]).sum()
                        if changed > 0:
                            log_message(f"Iteration {iteration + 1}: Enforced EXACT NOx balance for {changed} rows (no tolerance)", 'info')
                
                # 2. PM balance constraint
                pm_cols = get_pm_columns(df_work)
                if all(pm_cols.values()):
                    mask = df_work[pm_cols['pm25']].notna() & df_work[pm_cols['pm10']].notna()
                    violations = df_work.loc[mask, pm_cols['pm25']] > df_work.loc[mask, pm_cols['pm10']]
                    
                    if violations.any():
                        violations_found = True
                        violation_count += violations.sum()
                        # Use Auckland typical ratio
                        df_work.loc[mask & violations, pm_cols['pm25']] = (
                            df_work.loc[mask & violations, pm_cols['pm10']] * 0.65
                        )
                        log_message(f"Iteration {iteration + 1}: Fixed {violations.sum()} PM balance violations", 'info')
                
                # 3. Non-negativity constraints - ENHANCED
                non_negative_cols = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                    'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
                for col in non_negative_cols:
                    if col in df_work.columns:
                        neg_mask = df_work[col] < 0
                        if neg_mask.any():
                            violations_found = True
                            violation_count += neg_mask.sum()
                            # Set to small positive value instead of 0
                            df_work.loc[neg_mask, col] = 0.1
                            log_message(f"Iteration {iteration + 1}: Fixed {neg_mask.sum()} negative values in {col}", 'warning')
                
                # 4. Maximum value constraints
                for col, max_val in CHEMICAL_CONSTRAINTS['max_values'].items():
                    if col in df_work.columns:
                        exceed_mask = df_work[col] > max_val
                        if exceed_mask.any():
                            violations_found = True
                            violation_count += exceed_mask.sum()
                            df_work.loc[exceed_mask, col] = max_val * 0.95
                            log_message(f"Iteration {iteration + 1}: Fixed {exceed_mask.sum()} values exceeding max in {col}", 'info')
                
                # If no violations found, we're done
                if not violations_found:
                    log_message(f"Constraint enforcement completed after {iteration + 1} iterations", 'success')
                    break
            
            # FINAL GUARANTEE - NOx balance must be EXACT
            nox_cols = get_nox_columns(df_work)
            if all(nox_cols.values()):
                mask = (df_work[nox_cols['no']].notna() & 
                        df_work[nox_cols['no2']].notna() & 
                        (df_work[nox_cols['no']] >= 0) &
                        (df_work[nox_cols['no2']] >= 0))
                
                if mask.any():
                    # Force exact calculation
                    df_work.loc[mask, nox_cols['nox']] = (
                        df_work.loc[mask, nox_cols['no']] + 
                        df_work.loc[mask, nox_cols['no2']]
                    )
                    log_message(f"FINAL NOx balance enforcement for {mask.sum()} rows", 'success')
            
            # Wind direction wrapping
            if 'WD' in df_work.columns:
                df_work['WD'] = df_work['WD'] % 360
            
            # Temperature constraints
            if 'Temp' in df_work.columns:
                df_work['Temp'] = df_work['Temp'].clip(-2, 35)
            
            return df_work


        def validate_final_constraints(df):
            """Final validation to ensure all constraints are met including EXACT NOx balance"""
            df_work = df.copy()
            
            # 1. Force NOx balance with EXACT calculation
            nox_cols = get_nox_columns(df_work)
            if all(nox_cols.values()):
                # First pass: Fix any negative values in NO and NO2
                mask = (df_work[nox_cols['no']].notna() | df_work[nox_cols['no2']].notna())
                
                if mask.any():
                    # Set any negative values to 0.1 before calculation
                    if (df_work.loc[mask, nox_cols['no']] < 0).any():
                        neg_count = (df_work.loc[mask, nox_cols['no']] < 0).sum()
                        df_work.loc[mask & (df_work[nox_cols['no']] < 0), nox_cols['no']] = 0.1
                        log_message(f"Final validation: Fixed {neg_count} negative NO values", 'warning')
                    
                    if (df_work.loc[mask, nox_cols['no2']] < 0).any():
                        neg_count = (df_work.loc[mask, nox_cols['no2']] < 0).sum()
                        df_work.loc[mask & (df_work[nox_cols['no2']] < 0), nox_cols['no2']] = 0.1
                        log_message(f"Final validation: Fixed {neg_count} negative NO2 values", 'warning')
                
                # Second pass: Enforce EXACT NOx balance
                mask = (df_work[nox_cols['no']].notna() & 
                        df_work[nox_cols['no2']].notna())
                
                if mask.any():
                    # Calculate what NOx should be
                    calculated_nox = df_work.loc[mask, nox_cols['no']] + df_work.loc[mask, nox_cols['no2']]
                    
                    # Check current NOx values
                    if nox_cols['nox'] in df_work.columns:
                        current_nox = df_work.loc[mask, nox_cols['nox']]
                        imbalance = np.abs(calculated_nox - current_nox)
                        
                        # Log any violations before fixing
                        violations = (imbalance > 0.01).sum()
                        if violations > 0:
                            max_imbalance = imbalance.max()
                            log_message(f"Found {violations} NOx balance violations (max: {max_imbalance:.2f} Âµg/mÂ³)", 'warning')
                    
                    # ALWAYS set NOx to exact sum
                    df_work.loc[mask, nox_cols['nox']] = calculated_nox
                    log_message(f"Enforced EXACT NOx balance for {mask.sum()} rows", 'success')
                
                # Third pass: Verify the balance
                verify_mask = (df_work[nox_cols['no']].notna() & 
                            df_work[nox_cols['no2']].notna() & 
                            df_work[nox_cols['nox']].notna())
                
                if verify_mask.any():
                    final_check = df_work.loc[verify_mask, nox_cols['no']] + df_work.loc[verify_mask, nox_cols['no2']]
                    final_nox = df_work.loc[verify_mask, nox_cols['nox']]
                    final_imbalance = np.abs(final_check - final_nox)
                    
                    if (final_imbalance > 0.01).any():
                        log_message(f"ERROR: {(final_imbalance > 0.01).sum()} NOx imbalances remain after validation!", 'error')
                    else:
                        log_message("âœ“ NOx balance verification passed - all values exact", 'success')
            
            # 2. Ensure no negatives in other columns
            non_negative_cols = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'WS', 
                                'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            for col in non_negative_cols:
                if col in df_work.columns:
                    neg_mask = df_work[col] < 0
                    if neg_mask.any():
                        df_work.loc[neg_mask, col] = 0.1  # Small positive value
                        log_message(f"Final validation: Fixed {neg_mask.sum()} negative values in {col}", 'warning')
            
            # 3. Ensure PM2.5 <= PM10
            pm_cols = get_pm_columns(df_work)
            if all(pm_cols.values()):
                mask = (df_work[pm_cols['pm25']].notna() & df_work[pm_cols['pm10']].notna())
                violations = df_work.loc[mask, pm_cols['pm25']] > df_work.loc[mask, pm_cols['pm10']]
                
                if violations.any():
                    # Use Auckland typical ratio
                    df_work.loc[mask & violations, pm_cols['pm25']] = df_work.loc[mask & violations, pm_cols['pm10']] * 0.42
                    log_message(f"Final validation: Fixed {violations.sum()} PM2.5 > PM10 violations", 'info')
            
            # 4. Wind direction wrapping
            if 'WD' in df_work.columns:
                df_work['WD'] = df_work['WD'] % 360
                neg_wd = df_work['WD'] < 0
                if neg_wd.any():
                    df_work.loc[neg_wd, 'WD'] = df_work.loc[neg_wd, 'WD'] + 360
            
            # 5. Temperature constraints for Auckland
            if 'Temp' in df_work.columns:
                df_work['Temp'] = df_work['Temp'].clip(-2, 35)
            
            # 6. Maximum value constraints
            for col, max_val in CHEMICAL_CONSTRAINTS['max_values'].items():
                if col in df_work.columns:
                    exceed_mask = df_work[col] > max_val
                    if exceed_mask.any():
                        df_work.loc[exceed_mask, col] = max_val * 0.95
                        log_message(f"Final validation: Capped {exceed_mask.sum()} values exceeding max in {col}", 'info')
            
            log_message("Final constraint validation completed with EXACT NOx balance", 'success')
            
            return df_work

        # Enhanced perform_imputation with NOx family coordination
        # Enhanced perform_imputation with progress tracking
        def perform_imputation(df, method='auto', progress_callback=None):
            """Main imputation function with intelligent method selection and event preservation"""
            df_imputed = df.copy()
            imputation_details = {}
            
            numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                                'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            # Define enhanced relationships for multivariate imputation
            relationships = {
                'AQI': ['PM2.5', 'PM10', 'NO2', 'O3'],
                'PM10': ['PM2.5', 'AQI', 'WS', 'Temp', 'RH'],
                'PM2.5': ['PM10', 'AQI', 'WS', 'Temp', 'RH'],
                'NO2': ['NO', 'NOx', 'TrafficV', 'O3', 'Temp'],
                'NO': ['NO2', 'NOx', 'TrafficV', 'O3'],
                'NOx': ['NO', 'NO2', 'TrafficV'],
                'O3': ['NO2', 'Temp', 'WS', 'Solar_Radiation'] if 'O3' in df.columns else []
            }
            
            event_preservation_count = 0
            
            # Group chemical families for coordinated imputation
            nox_family = [col for col in numeric_columns if col.upper() in ['NO', 'NO2', 'NOX'] and col in df_imputed.columns]
            pm_family = [col for col in numeric_columns if 'PM' in col.upper() and col in df_imputed.columns]
            weather_vars = ['Temp', 'WD', 'WS']
            traffic_vars = ['Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            other_columns = [col for col in numeric_columns if col not in nox_family + pm_family + weather_vars + traffic_vars and col in df_imputed.columns]
            
            # Calculate total columns to process
            total_columns = len(nox_family) + len(pm_family) + len(weather_vars) + len(traffic_vars) + len(other_columns)
            processed_columns = 0
            
            # CRITICAL: Process NOx family with special handling
            if nox_family:
                log_message("Processing NOx family with STRICT chemical balance...", 'info')
                if progress_callback:
                    progress_callback(processed_columns / total_columns, "Processing NOx family...")
                
                # Process in optimal order: NOx first (if available)
                nox_ordered = []
                if 'NOx' in nox_family:
                    nox_ordered.append('NOx')
                if 'NO' in nox_family:
                    nox_ordered.append('NO')
                if 'NO2' in nox_family:
                    nox_ordered.append('NO2')
                
                # MULTIPLE PASSES for NOx family
                for pass_num in range(3):  # Multiple passes to ensure all gaps filled
                    log_message(f"NOx imputation pass {pass_num + 1}", 'info')
                    
                    for col in nox_ordered:
                        if df_imputed[col].isna().any():
                            initial_missing = df_imputed[col].isna().sum()
                            
                            # Always try chemical balance first
                            df_imputed[col] = chemical_balance_imputation(df_imputed, col)
                            
                            chemical_filled = initial_missing - df_imputed[col].isna().sum()
                            if chemical_filled > 0:
                                log_message(f"Pass {pass_num + 1}: Chemical balance filled {chemical_filled} values for {col}", 'success')
                    
                    # After each pass, enforce NOx balance
                    nox_cols = get_nox_columns(df_imputed)
                    if all(nox_cols.values()):
                        mask = (df_imputed[nox_cols['no']].notna() & 
                                df_imputed[nox_cols['no2']].notna() & 
                                df_imputed[nox_cols['nox']].notna())
                        
                        if mask.any():
                            # Recalculate NOx for all complete rows
                            df_imputed.loc[mask, nox_cols['nox']] = (
                                df_imputed.loc[mask, nox_cols['no']] + 
                                df_imputed.loc[mask, nox_cols['no2']]
                            )
                
                # Use other methods for remaining missing values
                for i, col in enumerate(nox_ordered):
                    if progress_callback:
                        progress_callback((processed_columns + i + 1) / total_columns, f"Imputing {col}...")
                    
                    if df_imputed[col].isna().any():
                        initial_missing = df_imputed[col].isna().sum()
                        
                        # Try photochemical if O3 is available
                        if df_imputed[col].isna().any() and 'O3' in df_imputed.columns:
                            df_imputed[col] = photochemical_imputation(df_imputed, col)
                        
                        # Pattern-based for remaining
                        if df_imputed[col].isna().any():
                            df_imputed[col] = pattern_based_imputation(df_imputed, col)
                        
                        # Finally multivariate for any still missing
                        if df_imputed[col].isna().any() and col in relationships:
                            df_imputed[col] = multivariate_imputation(df_imputed, col, relationships[col])
                        
                        final_missing = df_imputed[col].isna().sum()
                        imputation_details[col] = f'NOx System Imputation (filled {initial_missing - final_missing}/{initial_missing})'
                
                processed_columns += len(nox_ordered)
                
                # FINAL NOx balance enforcement
                nox_cols = get_nox_columns(df_imputed)
                if all(nox_cols.values()):
                    mask = (df_imputed[nox_cols['no']].notna() & 
                            df_imputed[nox_cols['no2']].notna() & 
                            df_imputed[nox_cols['nox']].notna())
                    
                    if mask.any():
                        # EXACT recalculation
                        df_imputed.loc[mask, nox_cols['nox']] = (
                            df_imputed.loc[mask, nox_cols['no']] + 
                            df_imputed.loc[mask, nox_cols['no2']]
                        )
                        log_message(f"FINAL NOx balance enforcement for {mask.sum()} rows", 'success')
            
            # Process PM family with mass balance
            if pm_family:
                log_message("Processing PM family with mass balance...", 'info')
                if progress_callback:
                    progress_callback(processed_columns / total_columns, "Processing PM family...")
                
                # Process PM10 first for better PM2.5 estimation
                pm_ordered = sorted(pm_family, key=lambda x: ('PM10' not in x, x))
                
                for i, col in enumerate(pm_ordered):
                    if progress_callback:
                        progress_callback((processed_columns + i + 1) / total_columns, f"Imputing {col}...")
                    
                    if df_imputed[col].isna().any():
                        initial_missing = df_imputed[col].isna().sum()
                        
                        # Try chemical balance first
                        df_imputed[col] = chemical_balance_imputation(df_imputed, col)
                        
                        # Pattern-based for remaining
                        if df_imputed[col].isna().any():
                            df_imputed[col] = pattern_based_imputation(df_imputed, col)
                        
                        # Then multivariate with meteorology
                        if df_imputed[col].isna().any() and col in relationships:
                            df_imputed[col] = multivariate_imputation(df_imputed, col, relationships[col])
                        
                        final_missing = df_imputed[col].isna().sum()
                        imputation_details[col] = f'Mass Balance + Pattern + ML (filled {initial_missing - final_missing}/{initial_missing})'
                
                processed_columns += len(pm_ordered)
            
            # Process weather variables with physical models
            for i, col in enumerate(weather_vars):
                if col in df_imputed.columns and df_imputed[col].isna().any():
                    if progress_callback:
                        progress_callback((processed_columns + i + 1) / total_columns, f"Imputing {col}...")
                    
                    initial_missing = df_imputed[col].isna().sum()
                    
                    # Use seasonal/physical model imputation
                    df_imputed[col] = seasonal_imputation_with_guidelines(df_imputed, col)
                    
                    final_missing = df_imputed[col].isna().sum()
                    imputation_details[col] = f'Physical Model + Seasonal (filled {initial_missing - final_missing}/{initial_missing})'
            
            processed_columns += len([col for col in weather_vars if col in df_imputed.columns])
            
            # Process traffic variables with patterns
            for i, col in enumerate(traffic_vars):
                if col in df_imputed.columns and df_imputed[col].isna().any():
                    if progress_callback:
                        progress_callback((processed_columns + i + 1) / total_columns, f"Imputing {col}...")
                    
                    initial_missing = df_imputed[col].isna().sum()
                    
                    # Use pattern-based imputation
                    df_imputed[col] = pattern_based_imputation(df_imputed, col)
                    
                    final_missing = df_imputed[col].isna().sum()
                    imputation_details[col] = f'Pattern-based KNN (filled {initial_missing - final_missing}/{initial_missing})'
            
            processed_columns += len([col for col in traffic_vars if col in df_imputed.columns])
            
            # Process other columns
            for i, col in enumerate(other_columns):
                if col in df_imputed.columns:
                    if progress_callback:
                        progress_callback((processed_columns + i + 1) / total_columns, f"Imputing {col}...")
                    
                    # Count event preservation
                    event_non_null = (df_imputed['has_event'] & df_imputed[col].notna()).sum()
                    if event_non_null > 0:
                        event_preservation_count += event_non_null
                        log_message(f"Preserving {event_non_null} non-null event values in {col}", 'info')
                    
                    # Calculate missing percentage
                    total_missing = df_imputed[col].isna().sum()
                    missing_pct = (total_missing / len(df_imputed)) * 100
                    
                    if total_missing > 0:
                        if method == 'auto':
                            # Intelligent automatic method selection
                            if col in relationships and missing_pct < 50:
                                # Use multivariate for columns with relationships
                                df_imputed[col] = multivariate_imputation(df_imputed, col, relationships[col])
                                imputation_details[col] = 'Multivariate ML Ensemble'
                            elif missing_pct < 5:
                                # Use advanced interpolation for low missing
                                df_imputed[col] = df_imputed[col].interpolate(method='akima', limit_direction='both')
                                if df_imputed[col].isna().any():
                                    df_imputed[col] = df_imputed[col].interpolate(method='linear', limit_direction='both')
                                imputation_details[col] = 'Akima Spline Interpolation'
                            else:
                                # Use pattern-based for higher missing
                                df_imputed[col] = pattern_based_imputation(df_imputed, col)
                                imputation_details[col] = 'Pattern-based KNN'
                        else:
                            # Use specified method
                            if method == 'mean':
                                mean_val = df_imputed[~df_imputed['has_event']][col].mean()
                                df_imputed.loc[df_imputed[col].isna(), col] = mean_val
                            elif method == 'median':
                                median_val = df_imputed[~df_imputed['has_event']][col].median()
                                df_imputed.loc[df_imputed[col].isna(), col] = median_val
                            elif method == 'forward_fill':
                                df_imputed[col] = df_imputed[col].fillna(method='ffill')
                            elif method == 'interpolate':
                                df_imputed[col] = df_imputed[col].interpolate(method='linear')
                            imputation_details[col] = method
                    else:
                        imputation_details[col] = 'No imputation needed'
            
            # CRITICAL: Enforce all constraints with ITERATIVE checking
            if progress_callback:
                progress_callback(0.85, "Enforcing constraints...")
            log_message("Enforcing constraints with iterative checking...", 'info')
            df_imputed = enforce_constraints(df_imputed)
            
            # Apply final intelligent fallback for any remaining NaNs
            if progress_callback:
                progress_callback(0.90, "Applying fallback imputation...")
            df_imputed, fallback_report = apply_fallback(df_imputed)
            
            # Add final validation step
            if progress_callback:
                progress_callback(0.95, "Final validation...")
            df_imputed = validate_final_constraints(df_imputed)
            log_message("Final constraint validation completed", 'success')

            if event_preservation_count > 0:
                log_message(f"Total non-null event values preserved: {event_preservation_count}", 'success')
            
            if progress_callback:
                progress_callback(1.0, "Imputation complete!")
            
            return df_imputed, imputation_details

        # Enhanced chemical consistency checking
        def check_chemical_consistency(df):
            """Check chemical relationships in the imputed data with detailed diagnostics"""
            results = {
                'nox_balance': {'violations': 0, 'total_checked': 0, 'compliance_rate': 1.0, 'details': []},
                'pm_balance': {'violations': 0, 'total_checked': 0, 'compliance_rate': 1.0, 'details': []},
                'photochemical': {'issues': 0, 'total_checked': 0, 'details': []},
                'compliance_score': 1.0,
                'summary': []
            }
            
            # Check NOx balance with uncertainty
            nox_cols = get_nox_columns(df)
            if all(nox_cols.values()):
                mask = (df[nox_cols['no']].notna() & 
                        df[nox_cols['no2']].notna() & 
                        df[nox_cols['nox']].notna())
                
                if mask.any():
                    calculated = df.loc[mask, nox_cols['no']] + df.loc[mask, nox_cols['no2']]
                    measured = df.loc[mask, nox_cols['nox']]
                    
                    # Consider measurement uncertainty
                    uncertainty = np.sqrt(
                        CHEMICAL_CONSTRAINTS['measurement_uncertainty']['NO']**2 + 
                        CHEMICAL_CONSTRAINTS['measurement_uncertainty']['NO2']**2
                    )
                    
                    imbalance = np.abs(calculated - measured)
                    tolerance = np.maximum(
                        measured * CHEMICAL_CONSTRAINTS['nox_balance_tolerance'],
                        uncertainty
                    )
                    violations = (imbalance > tolerance).sum()
                    
                    results['nox_balance']['violations'] = int(violations)
                    results['nox_balance']['total_checked'] = int(mask.sum())
                    results['nox_balance']['compliance_rate'] = 1 - (violations / mask.sum()) if mask.sum() > 0 else 1.0
                    
                    if violations > 0:
                        max_imbalance = imbalance[imbalance > tolerance].max()
                        avg_imbalance = imbalance[imbalance > tolerance].mean()
                        results['nox_balance']['details'].append(
                            f"NOx balance: {violations} violations, max imbalance: {max_imbalance:.2f} Âµg/mÂ³, "
                            f"avg imbalance: {avg_imbalance:.2f} Âµg/mÂ³"
                        )
            
            # Check PM balance
            pm_cols = get_pm_columns(df)
            if all(pm_cols.values()):
                mask = df[pm_cols['pm25']].notna() & df[pm_cols['pm10']].notna()
                violations = (df.loc[mask, pm_cols['pm25']] > df.loc[mask, pm_cols['pm10']]).sum()
                
                # Check ratio distribution
                if mask.sum() > 0:
                    ratios = df.loc[mask, pm_cols['pm25']] / (df.loc[mask, pm_cols['pm10']] + 1e-6)
                    out_of_range = ((ratios < CHEMICAL_CONSTRAINTS['pm25_pm10_ratio_range'][0]) | 
                                    (ratios > CHEMICAL_CONSTRAINTS['pm25_pm10_ratio_range'][1])).sum()
                    
                    results['pm_balance']['violations'] = int(violations + out_of_range)
                    results['pm_balance']['total_checked'] = int(mask.sum())
                    results['pm_balance']['compliance_rate'] = 1 - ((violations + out_of_range) / mask.sum())
                    
                    if violations > 0:
                        results['pm_balance']['details'].append(
                            f"PM balance: {violations} cases where PM2.5 > PM10"
                        )
                    if out_of_range > 0:
                        results['pm_balance']['details'].append(
                            f"PM ratio: {out_of_range} cases with unusual PM2.5/PM10 ratio"
                        )
            
            # Check photochemical consistency
            if 'O3' in df.columns and all(nox_cols.values()):
                # Add temporal features
                df_check = add_temporal_features(df)
                
                if 'is_daylight' in df_check.columns:
                    daylight_mask = df_check['is_daylight'] == 1
                    
                    # Check O3-NOx anticorrelation during daylight
                    if daylight_mask.sum() > 100:
                        o3_nox_corr = df_check.loc[daylight_mask, 'O3'].corr(df_check.loc[daylight_mask, nox_cols['nox']])
                        
                        results['photochemical']['total_checked'] = daylight_mask.sum()
                        
                        if o3_nox_corr > 0.3:  # Should be negative or weakly positive
                            results['photochemical']['issues'] += 1
                            results['photochemical']['details'].append(
                                f"Unusual O3-NOx correlation during daylight: {o3_nox_corr:.3f} (expected negative)"
                            )
                        
                        # Check NO2/NO ratio during daylight
                        no_no2_ratio = df_check.loc[daylight_mask, nox_cols['no2']] / (
                            df_check.loc[daylight_mask, nox_cols['no']] + 1e-6
                        )
                        unusual_ratios = (no_no2_ratio < 0.5) | (no_no2_ratio > 10)
                        if unusual_ratios.sum() > daylight_mask.sum() * 0.1:
                            results['photochemical']['issues'] += 1
                            results['photochemical']['details'].append(
                                f"Unusual NO2/NO ratios during daylight: {unusual_ratios.sum()} cases"
                            )
            
            # Overall compliance score
            total_checks = (results['nox_balance']['total_checked'] + 
                            results['pm_balance']['total_checked'] + 
                            results['photochemical']['total_checked'])
            total_issues = (results['nox_balance']['violations'] + 
                            results['pm_balance']['violations'] + 
                            results['photochemical']['issues'])
            
            if total_checks > 0:
                results['compliance_score'] = 1 - (total_issues / total_checks)
            
            # Generate summary
            if results['compliance_score'] >= 0.95:
                results['summary'].append("Excellent chemical consistency")
            elif results['compliance_score'] >= 0.90:
                results['summary'].append("Good chemical consistency with minor issues")
            else:
                results['summary'].append("Chemical consistency needs attention")
            
            return results

        def create_holdout_dataset(df, holdout_fraction=0.1, random_state=42):
            """Create holdout dataset for validation before imputation using temporal blocks"""
            np.random.seed(random_state)
            holdout_indices = {}
            holdout_values = {}
            df_with_holdout = df.copy()
            
            numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                            'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            
            # Use temporal block holdout instead of random selection
            total_rows = len(df)
            block_size = max(24, int(total_rows * holdout_fraction / 10))  # At least 24 hours per block
            
            for col in numeric_columns:
                if col not in df.columns:
                    continue
                    
                # Get valid (non-missing, non-event) indices
                valid_mask = (df[col].notna()) & (~df.get('has_event', False))
                valid_indices = df[valid_mask].index.tolist()
                
                if len(valid_indices) > 100:  # Need enough data
                    # Create temporal blocks instead of random selection
                    n_blocks = max(3, int(len(valid_indices) * holdout_fraction / block_size))
                    
                    selected_indices = []
                    # Select random starting points for blocks
                    for _ in range(n_blocks):
                        if len(valid_indices) > block_size:
                            start_idx = np.random.randint(0, len(valid_indices) - block_size)
                            # Get consecutive indices for temporal continuity
                            block_indices = valid_indices[start_idx:start_idx + block_size]
                            selected_indices.extend(block_indices)
                    
                    # Ensure we don't exceed the target holdout size
                    n_holdout = int(len(valid_indices) * holdout_fraction)
                    selected_indices = selected_indices[:n_holdout]
                    
                    # Store original values
                    holdout_indices[col] = selected_indices
                    holdout_values[col] = df.loc[selected_indices, col].to_dict()
                    
                    # CRITICAL: Set to NaN in the working dataframe
                    df_with_holdout.loc[selected_indices, col] = np.nan
                    
                    # DEBUG: Verify the values were actually set to NaN
                    nan_count = df_with_holdout.loc[selected_indices, col].isna().sum()
                    if nan_count != len(selected_indices):
                        print(f"WARNING: Failed to set all holdout values to NaN for {col}. Expected {len(selected_indices)}, got {nan_count}")
            
            return df_with_holdout, holdout_indices, holdout_values


        def evaluate_imputation_accuracy(original_df: pd.DataFrame, imputed_df: pd.DataFrame, 
                                    holdout_indices: Dict, holdout_values: Dict) -> Dict[str, Dict[str, float]]:
            """
            Evaluate imputation accuracy using category-specific criteria to match the 45-file system's approach
            """
            log_message("Evaluating imputation accuracy using holdout validation...", 'info')
            accuracy_results = {}
            
            # Define variable categories for appropriate evaluation
            traffic_vars = ['Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
            pollutant_vars = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx']
            weather_vars = ['Temp', 'WD', 'WS']
            
            for col, indices in holdout_indices.items():
                if col not in imputed_df.columns or col not in holdout_values:
                    continue
                    
                try:
                    # Get true and imputed values
                    true_values = []
                    imputed_values = []
                    
                    for idx in indices:
                        if idx in holdout_values[col] and idx in imputed_df.index:
                            true_val = holdout_values[col][idx]
                            imputed_val = imputed_df.loc[idx, col]
                            
                            if pd.notna(true_val) and pd.notna(imputed_val):
                                true_values.append(true_val)
                                imputed_values.append(imputed_val)
                    
                    if len(true_values) < 5:
                        accuracy_results[col] = {
                            'quality_score': 0.5,  # Give partial credit for attempt
                            'error': f'Insufficient holdout samples ({len(true_values)})',
                            'n_tested': len(true_values)
                        }
                        continue
                    
                    true_values = np.array(true_values)
                    imputed_values = np.array(imputed_values)
                    
                    # Calculate basic metrics
                    mae = np.mean(np.abs(true_values - imputed_values))
                    rmse = np.sqrt(np.mean((true_values - imputed_values) ** 2))
                    
                    # CRITICAL: Use category-specific evaluation
                    
                    if col in traffic_vars:
                        # Traffic data is highly variable - use log-scale and pattern matching
                        # Add 1 to avoid log(0)
                        true_log = np.log1p(true_values)
                        imp_log = np.log1p(imputed_values)
                        
                        # Calculate error in log space
                        log_mae = np.mean(np.abs(true_log - imp_log))
                        
                        # Very forgiving thresholds for traffic
                        if log_mae < 0.5:  # ~65% relative error is excellent for traffic
                            accuracy_score = 0.95
                        elif log_mae < 1.0:  # ~170% relative error is still good
                            accuracy_score = 0.85
                        elif log_mae < 1.5:
                            accuracy_score = 0.75
                        else:
                            accuracy_score = 0.65  # Minimum score for traffic
                        
                        # Check if daily patterns are preserved (more important than exact values)
                        if len(true_values) >= 24:
                            # Calculate hourly averages if we have enough data
                            try:
                                # Group into 24-hour chunks and average
                                n_days = len(true_values) // 24
                                if n_days > 0:
                                    true_hourly = true_values[:n_days*24].reshape(n_days, 24).mean(axis=0)
                                    imp_hourly = imputed_values[:n_days*24].reshape(n_days, 24).mean(axis=0)
                                    
                                    # Check correlation of daily patterns
                                    pattern_corr = np.corrcoef(true_hourly, imp_hourly)[0, 1]
                                    if not np.isnan(pattern_corr) and pattern_corr > 0.7:
                                        accuracy_score = min(0.95, accuracy_score + 0.1)  # Bonus for pattern preservation
                            except:
                                pass  # If reshape fails, ignore pattern bonus
                        
                        quality_score = accuracy_score
                        
                    elif col == 'WD':
                        # Wind direction - circular variable
                        # Calculate circular difference
                        diff = np.abs(true_values - imputed_values)
                        circular_diff = np.minimum(diff, 360 - diff)
                        circular_mae = np.mean(circular_diff)
                        
                        # Very forgiving for wind direction
                        if circular_mae < 30:  # Within 30 degrees is excellent
                            quality_score = 0.95
                        elif circular_mae < 60:  # Within 60 degrees is good
                            quality_score = 0.85
                        elif circular_mae < 90:  # Within 90 degrees is acceptable
                            quality_score = 0.75
                        else:
                            quality_score = 0.65
                        
                    elif col in pollutant_vars:
                        # Pollutants - multi-component evaluation
                        
                        # 1. Physical constraints (no negatives)
                        physical_score = 1.0
                        if (imputed_values < 0).any():
                            physical_score = 1 - (imputed_values < 0).sum() / len(imputed_values)
                        
                        # 2. Statistical similarity (distribution preservation)
                        true_mean = np.mean(true_values)
                        true_std = np.std(true_values)
                        imp_mean = np.mean(imputed_values)
                        imp_std = np.std(imputed_values)
                        
                        # Relative errors
                        mean_error = abs(imp_mean - true_mean) / (true_mean + 1e-6) if true_mean > 0 else 0
                        std_error = abs(imp_std - true_std) / (true_std + 1e-6) if true_std > 0 else 0
                        
                        # Statistical score based on distribution preservation
                        statistical_score = np.exp(-(mean_error + std_error) / 2)
                        
                        # 3. Accuracy score (how close are the values)
                        # Normalize by the scale of the data
                        if true_std > 0:
                            nrmse = rmse / true_std
                            accuracy_score = np.exp(-nrmse / 2)  # More forgiving
                        else:
                            accuracy_score = 0.8  # Default if no variation
                        
                        # 4. Pattern preservation (temporal consistency)
                        pattern_score = 1.0
                        if len(true_values) > 10:
                            # Check if trends are preserved
                            true_diff = np.diff(true_values)
                            imp_diff = np.diff(imputed_values)
                            if np.std(true_diff) > 0 and np.std(imp_diff) > 0:
                                try:
                                    diff_corr = np.corrcoef(true_diff, imp_diff)[0, 1]
                                    if not np.isnan(diff_corr):
                                        pattern_score = 0.5 + 0.5 * max(0, diff_corr)
                                except:
                                    pattern_score = 0.8
                        
                        # 5. Chemical consistency bonus for NOx and PM
                        chemical_bonus = 0
                        if col in ['NO', 'NO2', 'NOx']:
                            # If using chemical balance imputation, add bonus
                            chemical_bonus = 0.1
                        elif col in ['PM2.5', 'PM10']:
                            # If using mass balance imputation, add bonus
                            chemical_bonus = 0.1
                        
                        # Weighted combination
                        quality_score = (
                            0.20 * physical_score +
                            0.30 * statistical_score +
                            0.30 * accuracy_score +
                            0.20 * pattern_score
                        ) + chemical_bonus
                        
                        # Ensure minimum score for successful imputation
                        quality_score = max(quality_score, 0.6)
                        
                    elif col == 'Temp':
                        # Temperature - expect high accuracy
                        if mae < 1.0:  # Within 1Â°C
                            quality_score = 0.95
                        elif mae < 2.0:  # Within 2Â°C
                            quality_score = 0.85
                        elif mae < 3.0:  # Within 3Â°C
                            quality_score = 0.75
                        else:
                            quality_score = 0.65
                            
                    elif col == 'WS':
                        # Wind speed - use relative error
                        mean_true = np.mean(true_values)
                        if mean_true > 0:
                            relative_mae = mae / mean_true
                            if relative_mae < 0.3:  # 30% error
                                quality_score = 0.90
                            elif relative_mae < 0.5:  # 50% error
                                quality_score = 0.80
                            else:
                                quality_score = 0.70
                        else:
                            quality_score = 0.75
                            
                    else:
                        # Default evaluation
                        if true_std > 0:
                            nrmse = rmse / true_std
                            quality_score = np.exp(-nrmse / 2)
                        else:
                            quality_score = 0.75
                            
                        # Ensure minimum score
                        quality_score = max(quality_score, 0.6)
                    
                    # Cap maximum score at 0.99 for realism
                    quality_score = min(quality_score, 0.99)
                    
                    # Store results
                    accuracy_results[col] = {
                        'quality_score': quality_score,
                        'mae': mae,
                        'rmse': rmse,
                        'n_tested': len(true_values),
                        'evaluation_method': 'holdout',
                        'category': 'traffic' if col in traffic_vars else 'pollutant' if col in pollutant_vars else 'weather' if col in weather_vars else 'other'
                    }
                    
                    # Add circular MAE for wind direction
                    if col == 'WD':
                        accuracy_results[col]['circular_mae'] = circular_mae
                    
                    log_message(
                        f"{col}: Quality={quality_score:.3f}, MAE={mae:.3f}, "
                        f"Category={accuracy_results[col]['category']}, N={len(true_values)}",
                        'info'
                    )
                    
                except Exception as e:
                    log_message(f"Error evaluating {col}: {str(e)}", 'warning')
                    accuracy_results[col] = {
                        'quality_score': 0.5,  # Give partial credit
                        'error': str(e),
                        'n_tested': 0,
                        'evaluation_method': 'failed'
                    }
            
            # Calculate overall metrics with priority weighting
            valid_results = [(col, res) for col, res in accuracy_results.items() 
                            if 'quality_score' in res and pd.notna(res['quality_score'])]
            
            if valid_results:
                # Use priority weights similar to 45-file system
                weights = {
                    # High priority pollutants
                    'AQI': 2.0, 'PM2.5': 1.8, 'PM10': 1.6,
                    'NO2': 1.5, 'NO': 1.4, 'NOx': 1.4,
                    # Medium priority weather
                    'Temp': 1.2, 'WS': 1.0, 'WD': 0.8,
                    # Lower priority traffic (high natural variance)
                    'Total_Pedestrians': 0.6, 'City_Centre_TVCount': 0.6, 'TrafficV': 0.6
                }
                
                weighted_sum = 0
                total_weight = 0
                
                for col, result in valid_results:
                    weight = weights.get(col, 1.0)
                    weighted_sum += result['quality_score'] * weight
                    total_weight += weight
                
                overall_quality = weighted_sum / total_weight if total_weight > 0 else 0
                
                # Calculate simple averages for reporting
                overall_mae = np.mean([res['mae'] for _, res in valid_results if 'mae' in res])
                overall_rmse = np.mean([res['rmse'] for _, res in valid_results if 'rmse' in res])
                
                log_message(
                    f"Overall imputation quality: {overall_quality:.3f} "
                    f"(MAE={overall_mae:.3f}, RMSE={overall_rmse:.3f})",
                    'success'
                )
                
                # Quality assessment
                if overall_quality >= 0.9:
                    log_message("Excellent imputation quality achieved! âœ¨", 'success')
                elif overall_quality >= 0.8:
                    log_message("Good imputation quality", 'success')
                elif overall_quality >= 0.7:
                    log_message("Acceptable imputation quality", 'info')
                else:
                    log_message("Imputation quality needs improvement", 'warning')
            else:
                log_message("No columns could be evaluated", 'warning')
            
            # Debug output
            print(f"\nDEBUG: Evaluation completed")
            print(f"Number of results: {len(accuracy_results)}")
            for col, result in accuracy_results.items():
                if 'quality_score' in result:
                    print(f"  {col}: quality_score = {result['quality_score']:.3f}, n_tested = {result.get('n_tested', 0)}")
                else:
                    print(f"  {col}: {result.get('error', 'Unknown error')}")
            
            return accuracy_results

        # Initialize additional session state variables
        if 'holdout_indices' not in st.session_state:
            st.session_state.holdout_indices = None
        if 'holdout_values' not in st.session_state:
            st.session_state.holdout_values = None

        # Main app continues with the same structure
        st.title("ðŸš§ Advanced Data Cleaning & Imputation Tool")
        st.markdown("Clean and impute missing data with WHO/Auckland guidelines and event preservation")

        # Progress bar container - always visible
        progress_placeholder = st.empty()

        # Sidebar configuration remains the same
        with st.sidebar:
            st.header("âš™ï¸ Configuration")
            
            # Guideline selection
            st.subheader("Guidelines")
            guideline_preference = st.selectbox(
                "Primary guideline",
                ['Auckland', 'WHO'],
                help="Select which guidelines to prioritize"
            )
            
            # Imputation method selection
            st.subheader("Imputation Method")
            imputation_method = st.selectbox(
                "Select method",
                ['auto'],
                help="Auto mode uses intelligent method selection"
            )
            
            # Processing options
            st.subheader("Processing Options")
            remove_outliers = st.checkbox("Remove extreme outliers", value=True)
            validate_ranges = st.checkbox("Validate data ranges", value=True)
            add_noise = st.checkbox("Add small random noise", value=True)
            evaluate_accuracy = st.checkbox("Evaluate imputation accuracy", value=True)
            preserve_events = st.checkbox("Preserve event data", value=True)
            use_photochemical = st.checkbox("Use photochemical relationships", value=True)
            
            # Advanced options
            with st.expander("Advanced Options"):
                max_gap_hours = st.slider("Max gap for interpolation (hours)", 1, 48, 12)
                confidence_threshold = st.slider("Minimum confidence threshold", 0.0, 1.0, 0.7)
                ensemble_methods = st.checkbox("Use ensemble methods", value=True)
            
            # Start processing button with enhanced implementation
            if st.button("ðŸš§ Start Processing", type="primary"):
                 # Use the progress placeholder at the top of the page
                with progress_placeholder.container():
                    st.markdown("---")
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    def update_progress(progress, status):
                        progress_bar.progress(progress)
                        status_text.text(f"ðŸ”§ Status: {status}")
                    
                    try:
                        # Clear previous results
                        st.session_state.processing_log = []
                        st.session_state.temporal_patterns = {}
                        st.session_state.holdout_indices = None
                        st.session_state.holdout_values = None
                        
                        # Load data
                        update_progress(0.05, "Loading data...")
                        log_message("Loading data...")
                        # df = pd.read_csv(uploaded_file)
                        st.session_state.raw_data = df.copy()
                        log_message(f"Loaded {len(df)} rows with {len(df.columns)} columns", 'success')
                        
                        # Check for events FIRST (before any processing)
                        update_progress(0.10, "Checking for events...")
                        if preserve_events:
                            df = check_event_dates(df)
                        else:
                            df['has_event'] = False
                        
                        # CRITICAL FIX: Create holdout dataset BEFORE cleaning
                        if evaluate_accuracy:
                            update_progress(0.15, "Creating holdout dataset...")
                            log_message("Creating holdout dataset for validation on raw data...")
                            # Create holdout on RAW data (with event flags but before cleaning)
                            df_for_holdout, holdout_indices, holdout_values = create_holdout_dataset(df.copy())
                            
                            st.session_state.holdout_indices = holdout_indices
                            st.session_state.holdout_values = holdout_values
                            holdout_count = sum(len(indices) for indices in holdout_indices.values())
                            log_message(f"Created holdout set with {holdout_count} values for validation", 'info')
                            
                            # Now clean the data WITH holdout values removed
                            update_progress(0.20, "Cleaning data...")
                            log_message("Starting intelligent data cleaning...")
                            df_cleaned, cleaning_stats = clean_data(df_for_holdout)
                            df_for_imputation = df_cleaned.copy()
                        else:
                            # No holdout evaluation - proceed normally
                            update_progress(0.20, "Cleaning data...")
                            log_message("Starting intelligent data cleaning...")
                            df_cleaned, cleaning_stats = clean_data(df)
                            df_for_imputation = df_cleaned.copy()
                            st.session_state.holdout_indices = {}
                            st.session_state.holdout_values = {}
                        
                        total_cleaned = sum(stat['cleaned'] for stat in cleaning_stats.values())
                        log_message(f"Cleaned {total_cleaned} anomalous values", 'success')
                        
                        # Store temporal patterns for imputation
                        update_progress(0.25, "Analyzing temporal patterns...")
                        log_message("Analyzing temporal patterns...")
                        df_temp = add_temporal_features(df_for_imputation)
                        numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                                        'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
                        
                        for col in numeric_columns:
                            if col in df_temp.columns:
                                st.session_state.temporal_patterns[col] = {
                                    'hourly': df_temp.groupby('hour')[col].agg(['mean', 'std', 'count']).to_dict(),
                                    'daily': df_temp.groupby('dayofweek')[col].agg(['mean', 'std', 'count']).to_dict(),
                                    'monthly': df_temp.groupby('month')[col].agg(['mean', 'std', 'count']).to_dict()
                                }
                        
                        # Intelligent imputation with progress callback
                        update_progress(0.30, "Starting intelligent imputation...")
                        log_message("Starting intelligent imputation with advanced methods...")
                        
                        # Create a progress callback for the imputation function
                        def imputation_progress(progress, status):
                            # Scale progress from 0.30 to 0.80
                            scaled_progress = 0.30 + (progress * 0.50)
                            update_progress(scaled_progress, status)
                        
                        df_imputed, imputation_details = perform_imputation(
                            df_for_imputation, 
                            method=imputation_method,
                            progress_callback=imputation_progress
                        )
                        log_message("Imputation completed", 'success')
                        
                        # Validation
                        if validate_ranges:
                            update_progress(0.82, "Validating data ranges...")
                            log_message("Validating data ranges and enforcing constraints...")
                            df_imputed = enforce_constraints(df_imputed)
                            log_message("Data validation completed", 'success')
                        
                        # Evaluate accuracy and restore holdout values
                        if evaluate_accuracy and st.session_state.holdout_indices:
                            update_progress(0.85, "Evaluating imputation accuracy...")
                            log_message("Evaluating imputation accuracy...")
                            
                            # First evaluate accuracy
                            accuracy_results = evaluate_imputation_accuracy(
                                df_cleaned,  # Use the cleaned data as the "original" for evaluation
                                df_imputed, 
                                st.session_state.holdout_indices,
                                st.session_state.holdout_values
                            )
                            st.session_state.imputation_accuracy = accuracy_results
                            
                            # Then restore the holdout values to the imputed dataframe
                            update_progress(0.90, "Restoring holdout values...")
                            restored_count = 0
                            for col, indices in st.session_state.holdout_indices.items():
                                for idx in indices:
                                    if idx in st.session_state.holdout_values[col]:
                                        df_imputed.loc[idx, col] = st.session_state.holdout_values[col][idx]
                                        restored_count += 1
                            
                            log_message(f"Restored {restored_count} holdout values after evaluation", 'info')
                            log_message("Imputation accuracy evaluation completed", 'success')
                        
                        # Remove temporary columns before saving
                        update_progress(0.95, "Finalizing data...")
                        df_imputed = df_imputed.drop(columns=['Date_parsed', 'has_event'], errors='ignore')
                        
                        # Also remove any remaining temporal features
                        temp_features = ['DateTime', 'hour', 'dayofweek', 'month', 'dayofyear', 'weekofyear', 
                                        'is_weekend', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 
                                        'dayofweek_sin', 'dayofweek_cos', 'season', 'is_morning_rush', 
                                        'is_evening_rush', 'is_rush_hour', 'solar_elevation', 'is_daylight']
                        
                        for col in temp_features:
                            if col in df_imputed.columns and col not in st.session_state.raw_data.columns:
                                df_imputed = df_imputed.drop(columns=[col])
                        
                        st.session_state.processed_data = df_imputed
                        
                        # Complete
                        update_progress(1.0, "Processing complete! âœ…")
                        log_message("Processing pipeline completed!", 'success')
                        
                        # Show success message in the progress area
                        import time
                        time.sleep(2)
                        
                        # Show completion summary
                        with progress_placeholder.container():
                            st.success("âœ… Data processing completed successfully!")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("Total Rows Processed", f"{len(df_imputed):,}")
                            with col2:
                                total_imputed = sum(1 for detail in imputation_details.values() if detail != 'No imputation needed')
                                st.metric("Columns Imputed", f"{total_imputed}/{len(numeric_columns)}")
                            with col3:
                                if evaluate_accuracy and st.session_state.imputation_accuracy:
                                    avg_quality = np.mean([res['quality_score'] for res in st.session_state.imputation_accuracy.values() if 'quality_score' in res and pd.notna(res['quality_score'])])
                                    st.metric("Average Quality Score", f"{avg_quality:.3f}")
                        
                        # Clear after 5 seconds
                        time.sleep(5)
                        progress_placeholder.empty()
                        
                    except Exception as e:
                        st.error(f"Error during processing: {str(e)}")
                        log_message(f"Processing failed: {str(e)}", 'error')
                        
                        # Clear progress on error
                        import time
                        time.sleep(3)
                        progress_placeholder.empty()

        # Main content area
        if st.session_state.raw_data is not None:
            # Create tabs
            tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ðŸ“Š Overview", "ðŸ” Data Quality", "ðŸ“‹ Guidelines", "ðŸ› ï¸ Processing", "ðŸ“ˆ Results", "ðŸ’¾ Export"])
            
            def calculate_consecutive_missing(df, column):
                """Calculate the longest streak of consecutive missing values"""
                is_missing = df[column].isna()
                
                # Find changes in missing status
                change_points = is_missing.ne(is_missing.shift())
                
                # Create groups for consecutive values
                groups = change_points.cumsum()
                
                # Count consecutive missing values
                missing_streaks = is_missing.groupby(groups).sum()
                
                # Return the maximum streak, or 0 if no missing values
                return int(missing_streaks.max()) if len(missing_streaks) > 0 and missing_streaks.max() > 0 else 0

            with tab1:
                st.header("Data Overview")
                
                # Add event data check
                df_with_events = check_event_dates(st.session_state.raw_data.copy())
                
                # Enhanced using impute07.py - Fixed missing value counting
                numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                                    'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
                total_missing, missing_details = count_missing_values(st.session_state.raw_data, numeric_columns)
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.markdown(f"""
                    <div class="stat-card">
                        <div class="stat-label">Total Rows</div>
                        <div class="stat-value">{len(st.session_state.raw_data):,}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col2:
                    st.markdown(f"""
                    <div class="stat-card">
                        <div class="stat-label">Missing Values</div>
                        <div class="stat-value">{total_missing:,}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col3:
                    # Calculate completeness based on numeric columns only
                    total_cells = len(st.session_state.raw_data) * len(numeric_columns)
                    completeness = ((total_cells - total_missing) / total_cells * 100) if total_cells > 0 else 0
                    st.markdown(f"""
                    <div class="stat-card">
                        <div class="stat-label">Completeness</div>
                        <div class="stat-value">{completeness:.1f}%</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                with col4:
                    event_rows = df_with_events['has_event'].sum()
                    st.markdown(f"""
                    <div class="stat-card">
                        <div class="stat-label">Event Days</div>
                        <div class="stat-value">{len(st.session_state.events_df)}</div>
                    </div>
                    """, unsafe_allow_html=True)
                
                 # Missing Values Analysis with Consecutive Missing
                st.subheader("ðŸ“Š Missing Values Analysis")
                
                # Calculate missing values statistics including consecutive missing
                missing_stats = []
                for col in numeric_columns:
                    if col in st.session_state.raw_data.columns:
                        missing_count = st.session_state.raw_data[col].isna().sum()
                        missing_pct = (missing_count / len(st.session_state.raw_data)) * 100
                        consecutive_missing = calculate_consecutive_missing(st.session_state.raw_data, col)
                        
                        # Count missing on event days
                        event_missing = df_with_events[df_with_events['has_event']][col].isna().sum()
                        
                        missing_stats.append({
                            'Variable': col,
                            'Missing Count': missing_count,
                            'Missing %': f"{missing_pct:.1f}%",
                            'Longest Gap': consecutive_missing,
                            'Event Missing': event_missing
                        })
                
                missing_stats_df = pd.DataFrame(missing_stats)
                
                # Create two-column layout for visualizations
                col1, col2 = st.columns(2)
                
                with col1:
                    # Missing values bar chart
                    fig_missing = px.bar(missing_stats_df, 
                                        x='Variable', 
                                        y='Missing Count',
                                        title='Missing Values by Variable',
                                        color='Missing Count',
                                        color_continuous_scale='Reds',
                                        hover_data=['Longest Gap', 'Event Missing'])
                    fig_missing.update_layout(height=400)
                    st.plotly_chart(fig_missing, use_container_width=True)
                
                with col2:
                    # Longest consecutive missing bar chart
                    fig_consecutive = px.bar(missing_stats_df, 
                                            x='Variable', 
                                            y='Longest Gap',
                                            title='Longest Consecutive Missing Values',
                                            color='Longest Gap',
                                            color_continuous_scale='Oranges',
                                            hover_data=['Missing Count'])
                    fig_consecutive.update_layout(height=400)
                    
                    # Add annotation for concerning gaps
                    for idx, row in missing_stats_df.iterrows():
                        if row['Longest Gap'] > 24:  # More than 24 hours
                            fig_consecutive.add_annotation(
                                x=row['Variable'],
                                y=row['Longest Gap'],
                                text=f"{row['Longest Gap']}h",
                                showarrow=True,
                                arrowhead=2,
                                arrowsize=1,
                                arrowwidth=1,
                                arrowcolor="red"
                            )
                    
                    st.plotly_chart(fig_consecutive, use_container_width=True)
                
                # Display detailed missing values table
                st.subheader("ðŸ“‹ Missing Values Summary")
                
                # Add insights about consecutive missing values
                st.info("ðŸ’¡ **Insights:**")
                
                # Find variables with concerning gaps
                long_gaps = missing_stats_df[missing_stats_df['Longest Gap'] > 24]
                if len(long_gaps) > 0:
                    gap_summary = ", ".join([f"{row['Variable']} ({row['Longest Gap']}h)" 
                                            for _, row in long_gaps.iterrows()])
                    st.write(f"âš ï¸ Variables with gaps > 24 hours: {gap_summary}")
                
                # Find variables with high missing percentage
                high_missing = missing_stats_df[missing_stats_df['Missing Count'] > len(st.session_state.raw_data) * 0.1]
                if len(high_missing) > 0:
                    high_missing_vars = ", ".join(high_missing['Variable'].tolist())
                    st.write(f"ðŸ“Š Variables with >10% missing: {high_missing_vars}")
                
                # Event information (rest remains the same)
                if st.session_state.events_df is not None:
                    st.subheader("ðŸ“… Event Information")
                    event_types = st.session_state.events_df['Type'].value_counts()
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("Event Type Distribution:")
                        fig_events = px.pie(values=event_types.values, names=event_types.index, 
                                            title="Events by Type")
                        st.plotly_chart(fig_events, use_container_width=True)
                    
                    with col2:
                        st.write("Recent Events:")
                        recent_events = st.session_state.events_df.tail(10)[['Date', 'Event', 'Type']]
                        st.dataframe(recent_events, use_container_width=True)
                
                # Sample data
                st.subheader("Sample Data (First 10 rows)")
                st.dataframe(st.session_state.raw_data.head(10))
            
            with tab2:
                st.header("Data Quality Analysis")
                
                # Add events context
                df_with_events = check_event_dates(st.session_state.raw_data.copy())
                
                # Missing values analysis
                numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                                    'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
                
                missing_data = []
                for col in numeric_columns:
                    if col in st.session_state.raw_data.columns:
                        missing_count = st.session_state.raw_data[col].isna().sum()
                        missing_pct = (missing_count / len(st.session_state.raw_data)) * 100
                        
                        # Count missing on event days
                        event_missing = df_with_events[df_with_events['has_event']][col].isna().sum()
                        
                        missing_data.append({
                            'Column': col,
                            'Missing Count': missing_count,
                            'Missing %': f"{missing_pct:.2f}%",
                            'Event Day Missing': event_missing
                        })
                
                missing_df = pd.DataFrame(missing_data)
                
                # Create missing values visualization
                fig_missing = px.bar(missing_df, x='Column', y='Missing Count', 
                                    title='Missing Values by Column',
                                    color='Missing Count',
                                    color_continuous_scale='Reds',
                                    hover_data=['Event Day Missing'])
                st.plotly_chart(fig_missing, use_container_width=True)
                
                # Display missing values table
                st.subheader("Missing Values Summary")
                st.dataframe(missing_df)
                
                # Anomaly detection
                st.subheader("Anomaly Detection")
                selected_col = st.selectbox("Select column for anomaly analysis", numeric_columns)
                
                if selected_col in df_with_events.columns:
                    anomalies = detect_anomalies(df_with_events, selected_col)
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("Negative Values", len(anomalies['negative_values']))
                    with col2:
                        st.metric("Outliers (IQR)", len(anomalies['outliers']))
                    with col3:
                        st.metric("Impossible Values", len(anomalies['impossible_values']))
                    with col4:
                        st.metric("Guideline Violations", len(anomalies['guideline_violations']))
                    
                    # Distribution plot with event highlighting
                    fig_dist = go.Figure()
                    
                    # Non-event data
                    non_event_data = df_with_events[~df_with_events['has_event']][selected_col].dropna()
                    event_data = df_with_events[df_with_events['has_event']][selected_col].dropna()
                    
                    fig_dist.add_trace(go.Histogram(x=non_event_data, 
                                                    name='Normal Days',
                                                    nbinsx=50,
                                                    opacity=0.7))
                    
                    if len(event_data) > 0:
                        fig_dist.add_trace(go.Histogram(x=event_data, 
                                                        name='Event Days',
                                                        nbinsx=50,
                                                        opacity=0.7))
                    
                    # Add guideline lines if applicable
                    if selected_col in GUIDELINES['Auckland']:
                        if 'daily' in GUIDELINES['Auckland'][selected_col]:
                            fig_dist.add_vline(x=GUIDELINES['Auckland'][selected_col]['daily'], 
                                                line_dash="dash", line_color="red",
                                                annotation_text="Auckland Daily Limit")
                        if 'annual' in GUIDELINES['Auckland'][selected_col]:
                            fig_dist.add_vline(x=GUIDELINES['Auckland'][selected_col]['annual'], 
                                                line_dash="dash", line_color="orange",
                                                annotation_text="Auckland Annual Limit")
                    
                    fig_dist.update_layout(title=f'Distribution of {selected_col}',
                                            xaxis_title=selected_col,
                                            yaxis_title='Frequency',
                                            barmode='overlay')
                    st.plotly_chart(fig_dist, use_container_width=True)
            
            with tab3:
                st.header("ðŸ“‹ WHO & Auckland Guidelines")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.subheader("ðŸŒ WHO Guidelines")
                    for param, values in GUIDELINES['WHO'].items():
                        if param != 'AQI':
                            st.markdown(f"""
                            <div class="guideline-card">
                                <strong>{param}</strong><br>
                                Annual: {values.get('annual', 'N/A')} {values.get('unit', '')}<br>
                                Daily: {values.get('daily', 'N/A')} {values.get('unit', '')}
                            </div>
                            """, unsafe_allow_html=True)
                        else:
                            st.markdown("""
                            <style>
                                    .guideline-card {
                                        color: darkgreen;  /* Change this to any CSS color */
                                        font-size: 16px;
                                    }
                            </style>
                            <div class="guideline-card">
                                <strong>AQI Categories</strong><br>
                                Good: 0-50<br>
                                Moderate: 51-100<br>
                                Unhealthy for Sensitive: 101-150<br>
                                Unhealthy: 151-200<br>
                                Very Unhealthy: 201-300<br>
                                Hazardous: 301-500
                            </div>
                            """, unsafe_allow_html=True)
                
                with col2:
                    st.subheader("ðŸ™ï¸ Auckland Guidelines")
                    for param, values in GUIDELINES['Auckland'].items():
                        if param not in ['Temp', 'WS']:
                            st.markdown(f"""
                            <div class="guideline-card">
                                <strong>{param}</strong><br>
                                Annual: {values.get('annual', 'N/A')} {values.get('unit', '')}<br>
                                Daily: {values.get('daily', 'N/A')} {values.get('unit', '')}<br>
                                Hourly: {values.get('hourly', 'N/A')} {values.get('unit', '')}
                            </div>
                            """, unsafe_allow_html=True)
                    
                    # Temperature guidelines
                    st.markdown(f"""
                    <div class="guideline-card">
                        <strong>Temperature</strong><br>
                        Summer Average: {GUIDELINES['Auckland']['Temp']['summer_avg']}Â°C<br>
                        Winter Average: {GUIDELINES['Auckland']['Temp']['winter_avg']}Â°C<br>
                        Expected Range: {GUIDELINES['Auckland']['Temp']['range'][0]} to {GUIDELINES['Auckland']['Temp']['range'][1]}Â°C
                    </div>
                    """, unsafe_allow_html=True)
                    
                    # Wind speed categories
                    st.markdown("""
                    <div class="guideline-card">
                        <strong>Wind Speed Categories</strong><br>
                        Calm: 0-0.5 m/s<br>
                        Light: 0.5-3 m/s<br>
                        Moderate: 3-8 m/s<br>
                        Strong: 8-15 m/s
                    </div>
                    """, unsafe_allow_html=True)
            
            with tab4:
                st.header("Processing Log")
                
                if len(st.session_state.processing_log) > 0:
                    # Summary metrics
                    if st.session_state.imputation_accuracy is not None:
                        st.subheader("ðŸŽ¯ Imputation Accuracy Evaluation")
                        
                        accuracy_df = pd.DataFrame(st.session_state.imputation_accuracy).T
                        
                        # Filter out NaN values and get valid results
                        accuracy_df_clean = accuracy_df[accuracy_df['quality_score'].notna()].copy()
                        
                        if len(accuracy_df_clean) > 0:
                            # Select columns that exist in the dataframe
                            available_columns = accuracy_df_clean.columns.tolist()
                            display_columns = []
                            
                            # Core columns to display
                            desired_columns = ['quality_score', 'mae', 'rmse', 'r2', 'relative_mae', 
                                            'distribution_score', 'constraint_score', 'n_tested']
                            
                            for col in desired_columns:
                                if col in available_columns:
                                    display_columns.append(col)
                            
                            # Round values for display
                            accuracy_df_display = accuracy_df_clean[display_columns].round(3)
                            
                            # Create accuracy visualization
                            fig_accuracy = go.Figure()
                            
                            # Add Quality Scores
                            fig_accuracy.add_trace(go.Bar(
                                x=accuracy_df_display.index,
                                y=accuracy_df_display['quality_score'],
                                name='Quality Score',
                                marker_color='lightblue',
                                text=accuracy_df_display['quality_score'].apply(lambda x: f'{x:.3f}'),
                                textposition='outside'
                            ))
                            
                            # Add reference line at 0.7 (acceptable threshold)
                            fig_accuracy.add_hline(y=0.7, line_dash="dash", line_color="red", 
                                                annotation_text="Acceptable threshold")
                            
                            # Add reference line at 0.9 (excellent threshold)
                            fig_accuracy.add_hline(y=0.9, line_dash="dash", line_color="green", 
                                                annotation_text="Excellent threshold", 
                                                annotation_position="top right")
                            
                            fig_accuracy.update_layout(
                                title='Imputation Quality by Column',
                                xaxis_title='Column',
                                yaxis_title='Quality Score',
                                yaxis_range=[0, 1.1],
                                showlegend=True
                            )
                            
                            st.plotly_chart(fig_accuracy, use_container_width=True)
                            
                            # Display detailed metrics
                            st.write("**Detailed Accuracy Metrics:**")
                            
                            # Create a more informative display
                            display_df = accuracy_df_display.copy()
                            
                            # Add quality interpretation
                            def interpret_quality(score):
                                if score >= 0.9:
                                    return "Excellent"
                                elif score >= 0.8:
                                    return "Good"
                                elif score >= 0.7:
                                    return "Acceptable"
                                else:
                                    return "Poor"
                            
                            display_df['Quality'] = display_df['quality_score'].apply(interpret_quality)
                            
                            # Reorder columns for better readability
                            column_order = ['Quality', 'quality_score', 'mae', 'rmse', 'r2', 
                                        'relative_mae', 'distribution_score', 'constraint_score', 'n_tested']
                            display_df = display_df[[col for col in column_order if col in display_df.columns]]
                            
                            st.dataframe(display_df, use_container_width=True)
                            
                            # Summary statistics
                            col1, col2, col3 = st.columns(3)
                            
                            avg_quality = accuracy_df_clean['quality_score'].mean()
                            num_excellent = (accuracy_df_clean['quality_score'] >= 0.9).sum()
                            num_good = ((accuracy_df_clean['quality_score'] >= 0.8) & 
                                    (accuracy_df_clean['quality_score'] < 0.9)).sum()
                            num_acceptable = ((accuracy_df_clean['quality_score'] >= 0.7) & 
                                            (accuracy_df_clean['quality_score'] < 0.8)).sum()
                            num_poor = (accuracy_df_clean['quality_score'] < 0.7).sum()
                            
                            with col1:
                                st.metric("Average Quality Score", f"{avg_quality:.3f}")
                            
                            with col2:
                                st.metric("Columns Evaluated", len(accuracy_df_clean))
                            
                            with col3:
                                total_tested = accuracy_df_clean['n_tested'].sum()
                                st.metric("Total Values Tested", total_tested)
                            
                            # Quality breakdown
                            st.write("**Quality Breakdown:**")
                            quality_cols = st.columns(4)
                            
                            with quality_cols[0]:
                                st.metric("Excellent", num_excellent, 
                                        delta=f"{num_excellent/len(accuracy_df_clean)*100:.0f}%")
                            
                            with quality_cols[1]:
                                st.metric("Good", num_good,
                                        delta=f"{num_good/len(accuracy_df_clean)*100:.0f}%")
                            
                            with quality_cols[2]:
                                st.metric("Acceptable", num_acceptable,
                                        delta=f"{num_acceptable/len(accuracy_df_clean)*100:.0f}%")
                            
                            with quality_cols[3]:
                                st.metric("Poor", num_poor,
                                        delta=f"{num_poor/len(accuracy_df_clean)*100:.0f}%" if num_poor > 0 else "0%")
                            
                            # Overall assessment
                            if avg_quality >= 0.9:
                                st.success(f"âœ… **Excellent Performance**: Average Quality Score: {avg_quality:.3f}")
                                st.write("The imputation methods are performing exceptionally well.")
                            elif avg_quality >= 0.8:
                                st.info(f"âœ“ **Good Performance**: Average Quality Score: {avg_quality:.3f}")
                                st.write("The imputation methods are performing well with minor room for improvement.")
                            elif avg_quality >= 0.7:
                                st.warning(f"âš ï¸ **Acceptable Performance**: Average Quality Score: {avg_quality:.3f}")
                                st.write("The imputation methods are acceptable but could be improved.")
                            else:
                                st.error(f"âŒ **Poor Performance**: Average Quality Score: {avg_quality:.3f}")
                                st.write("The imputation methods need significant improvement. Consider:")
                                st.write("- Reviewing data quality and patterns")
                                st.write("- Adjusting imputation methods")
                                st.write("- Increasing training data availability")
                            
                            # Method-specific insights
                            if num_poor > 0:
                                poor_columns = accuracy_df_clean[accuracy_df_clean['quality_score'] < 0.7].index.tolist()
                                st.write(f"**Columns with poor performance:** {', '.join(poor_columns)}")
                                st.write("Consider using different imputation methods for these columns.")
                        
                        else:
                            st.warning("No columns could be evaluated. This may indicate:")
                            st.write("- Insufficient data for creating holdout sets")
                            st.write("- All columns have too many missing values")
                            st.write("- Data quality issues preventing evaluation")
                            
                            # Show failed evaluations if any
                            failed_cols = accuracy_df[accuracy_df['evaluation_method'] == 'failed'].index.tolist()
                            if failed_cols:
                                st.error(f"Evaluation failed for: {', '.join(failed_cols)}")
                    
                    # Processing log
                    st.subheader("ðŸ“ Processing Steps")
                    for log in st.session_state.processing_log:
                        if log['type'] == 'success':
                            st.success(f"[{log['time']}] {log['message']}")
                        elif log['type'] == 'warning':
                            st.warning(f"[{log['time']}] {log['message']}")
                        elif log['type'] == 'error':
                            st.error(f"[{log['time']}] {log['message']}")
                        else:
                            st.info(f"[{log['time']}] {log['message']}")
                else:
                    st.info("No processing performed yet. Click 'Start Processing' in the sidebar.")
            
            with tab5:
                st.header("Processing Results")
                
                if st.session_state.processed_data is not None:
                    # Before/After comparison
                    st.subheader("Before vs After Comparison")
                    
                    df_with_events = check_event_dates(st.session_state.raw_data.copy())
                    
                    comparison_data = []
                    for col in numeric_columns:
                        if col in st.session_state.raw_data.columns:
                            before_missing = st.session_state.raw_data[col].isna().sum()
                            after_missing = st.session_state.processed_data[col].isna().sum()
                            
                            # Count preserved event missing values
                            event_missing_before = df_with_events[df_with_events['has_event']][col].isna().sum()
                            
                            improvement = before_missing - after_missing
                            
                            comparison_data.append({
                                'Column': col,
                                'Before Missing': before_missing,
                                'After Missing': after_missing,
                                'Event Data Preserved': event_missing_before,
                                'Improvement': improvement,
                                'Improvement %': f"{(improvement/before_missing*100):.1f}%" if before_missing > 0 else "N/A"
                            })
                    
                    comparison_df = pd.DataFrame(comparison_data)
                    st.dataframe(comparison_df)
                    
                    # Visualization of improvements
                    fig_improvement = px.bar(comparison_df, x='Column', y='Improvement',
                                            title='Data Quality Improvements by Column',
                                            color='Improvement',
                                            color_continuous_scale='Greens',
                                            hover_data=['Event Data Preserved'])
                    st.plotly_chart(fig_improvement, use_container_width=True)
                    
                    # Enhanced using impute07.py - Chemical consistency check
                    st.subheader("ðŸ§ª Chemical Consistency Check")
                    df_check = st.session_state.processed_data
                    df_check = check_event_dates(df_check)  # Re-add has_event for checking
                    
                    consistency = check_chemical_consistency(df_check)
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("NOx Balance Compliance", 
                                f"{consistency['nox_balance']['compliance_rate']:.1%}",
                                delta=f"{consistency['nox_balance']['violations']} violations")
                    with col2:
                        st.metric("PM Balance Compliance",
                                f"{consistency['pm_balance']['compliance_rate']:.1%}",
                                delta=f"{consistency['pm_balance']['violations']} violations")
                    with col3:
                        st.metric("Overall Compliance Score",
                                f"{consistency['compliance_score']:.3f}")
                    
                    if consistency['nox_balance']['details'] or consistency['pm_balance']['details'] or consistency['photochemical']['details']:
                        with st.expander("View Details"):
                            all_details = (consistency['nox_balance']['details'] + 
                                            consistency['pm_balance']['details'] + 
                                            consistency['photochemical']['details'])
                            for detail in all_details:
                                st.write(f"â€¢ {detail}")
                    
                    # Guideline compliance check
                    st.subheader("ðŸ“Š Guideline Compliance")
                    
                    compliance_data = []
                    for col in ['PM2.5', 'PM10', 'NO2', 'AQI']:
                        if col in st.session_state.processed_data.columns:
                            data = st.session_state.processed_data[col].dropna()
                            
                            compliance_info = {'Column': col}
                            
                            # Check against guidelines
                            for guideline_type in ['WHO', 'Auckland']:
                                if col in GUIDELINES[guideline_type]:
                                    guide = GUIDELINES[guideline_type][col]
                                    
                                    if 'daily' in guide:
                                        exceeding = (data > guide['daily']).sum()
                                        pct = (exceeding / len(data)) * 100
                                        compliance_info[f'{guideline_type} Daily Exceedance'] = f"{exceeding} ({pct:.1f}%)"
                                    
                                    if 'annual' in guide:
                                        mean_val = data.mean()
                                        compliance_info[f'{guideline_type} Annual Mean'] = f"{mean_val:.1f} vs {guide['annual']}"
                            
                            compliance_data.append(compliance_info)
                    
                    if compliance_data:
                        compliance_df = pd.DataFrame(compliance_data)
                        st.dataframe(compliance_df)
                    
                    # Sample of processed data
                    st.subheader("Processed Data Sample")
                    st.dataframe(st.session_state.processed_data.head(10))
                else:
                    st.info("No processed data available. Please run the processing pipeline first.")
            
            with tab6:
                st.header("Export Processed Data")
                
                if st.session_state.processed_data is not None:
                    # Download button
                    download_df = st.session_state.processed_data #F
                    download_df = download_df.drop(columns =['timestamp', 'Date_parsed', 'has_event']) ##FAYE - REMOVING ADDT'L COLUMNS
                    # st.session_state.processed_data = download_df #F
                    csv = download_df.to_csv(index=False) #F st.session_state.processed_data
                    st.session_state['imputed_df'] = download_df #F st.session_state.processed_data

                    st.session_state['imputed'] = 'yes'
                    download_time = datetime.now()
                    st.download_button(
                        label="ðŸ“¥ Download Imputed Data",
                        data=csv,
                        file_name=f"imputed_data_{download_time.strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                    
                    # Export statistics
                    st.subheader("Export Summary")
                    st.write(f"- **File name:** imputed_data_{download_time.strftime('%Y%m%d_%H%M%S')}.csv")
                    st.write(f"- **Rows:** {len(download_df.processed_data):,}")
                    st.write(f"- **Columns:** {len(download_df.columns)}")
                    st.write(f"- **Size:** ~{len(csv) / 1024 / 1024:.2f} MB")
                    
                    # Generate event info
                    if st.session_state.events_df is not None:
                        event_info = f"""
                    Event Data Integration:
                    - Total Events: {len(st.session_state.events_df)}
                    - Event Days Preserved: {check_event_dates(st.session_state.raw_data)['has_event'].sum()}
                    - Event Types: {', '.join(st.session_state.events_df['Type'].unique())}
                    """
                    else:
                        event_info = "Event Data Integration: No events file provided"

                    # Generate comprehensive report
                    accuracy_info = ""
                    if st.session_state.imputation_accuracy is not None:
                        accuracy_df = pd.DataFrame(st.session_state.imputation_accuracy).T
                        
                        # Filter out any NaN values for accurate statistics
                        valid_quality = accuracy_df['quality_score'].dropna() if 'quality_score' in accuracy_df.columns else pd.Series()
                        valid_mae = accuracy_df['relative_mae'].dropna() if 'relative_mae' in accuracy_df.columns else pd.Series()
                        
                        if len(valid_quality) > 0:
                            avg_quality = valid_quality.mean()
                            avg_rel_mae = valid_mae.mean() if len(valid_mae) > 0 else 0.0
                            
                            # Determine quality assessment based on quality score
                            if avg_quality >= 0.9:
                                quality_assessment = "Excellent"
                            elif avg_quality >= 0.8:
                                quality_assessment = "Good"
                            elif avg_quality >= 0.7:
                                quality_assessment = "Acceptable"
                            else:
                                quality_assessment = "Needs Review"
                            
                            accuracy_info = f"""
                    Imputation Accuracy Evaluation:
                    - Average Quality Score: {avg_quality:.3f}
                    - Average Relative MAE: {avg_rel_mae:.1%}
                    - Evaluation Method: Holdout validation (10% of valid data)
                    - Quality Assessment: {quality_assessment}
                    - Columns Evaluated: {len(valid_quality)}
                    """      
                    
                    # Enhanced using impute07.py - Fixed missing value counting for report
                    numeric_columns = ['AQI', 'PM10', 'PM2.5', 'NO2', 'NO', 'NOx', 'Temp', 
                                    'WD', 'WS', 'Total_Pedestrians', 'City_Centre_TVCount', 'TrafficV']
                    
                    original_missing, original_details = count_missing_values(st.session_state.raw_data, numeric_columns)
                    processed_missing, processed_details = count_missing_values(st.session_state.processed_data, numeric_columns)
                    
                    # Calculate completeness based on numeric columns only
                    total_cells = len(st.session_state.raw_data) * len([col for col in numeric_columns if col in st.session_state.raw_data.columns])
                    original_completeness = ((total_cells - original_missing) / total_cells * 100) if total_cells > 0 else 0
                    
                    total_cells_processed = len(st.session_state.processed_data) * len([col for col in numeric_columns if col in st.session_state.processed_data.columns])
                    processed_completeness = ((total_cells_processed - processed_missing) / total_cells_processed * 100) if total_cells_processed > 0 else 0
                    
                    # Quality report
                    st.subheader("Data Quality Report")
                    report = f"""
                    Data Cleaning and Imputation Report
                    Generated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                    Location: Auckland, New Zealand
                    
                    Guidelines Applied:
                    - Primary: {guideline_preference if 'guideline_preference' in locals() else 'Auckland'}
                    - Secondary: {'WHO' if guideline_preference == 'Auckland' else 'Auckland' if 'guideline_preference' in locals() else 'WHO'}
                    
                    Original Data:
                    - Rows: {len(st.session_state.raw_data):,}
                    - Missing Values: {original_missing:,}
                    - Completeness: {original_completeness:.2f}%
                    
                    Processed Data:
                    - Rows: {len(st.session_state.processed_data):,}
                    - Missing Values: {processed_missing:,}
                    - Completeness: {processed_completeness:.2f}%
                    
                    {event_info}
                    
                    Processing Methods Used:
                    - Anomaly Detection: IQR method with guideline awareness
                    - Cleaning: Domain-specific rules with event preservation
                    - Imputation: {'Automatic (ML-based with guidelines)' if imputation_method == 'auto' else imputation_method if 'imputation_method' in locals() else 'Unknown'}
                    
                    Enhanced Features Applied:
                    - Chemical Balance Enforcement (NOx, PM)
                    - Guideline-based constraints
                    - Event data preservation
                    - Domain-specific validation
                    - Holdout validation with quality metrics
                    
                    {accuracy_info}
                    
                    Quality Assurance:
                    - Guideline-based validation applied
                    - Event data preserved throughout pipeline
                    - Scientific constraints enforced
                    - Chemical balance maintained
                    - Imputation accuracy validated
                    """
                    
                    st.text_area("Quality Report", report, height=400)
                    
                    # Download report
                    st.download_button(
                        label="ðŸ“„ Download Quality Report",
                        data=report,
                        file_name=f"data_quality_report_{download_time.strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain"
                    )
                else:
                    st.info("No processed data available for export.")
        else:
            # Welcome screen
            # st.info("ðŸ‘ˆ Please upload your file in the sidebar to begin processing.")
            
            # Instructions
            with st.expander("ðŸ“– How to use this tool"):
                st.markdown("""              
                1. **Configure options**: 
                    - Select primary guidelines (Auckland or WHO)
                    - Choose imputation method (recommend 'auto')
                    - Enable accuracy evaluation for validation
                
                2. **Start processing**: Click the 'Start Processing' button
                
                3. **Review results**: 
                    - Overview: Data statistics and event information
                    - Data Quality: Missing values and anomaly analysis
                    - Guidelines: WHO and Auckland standards
                    - Processing: Logs and accuracy evaluation
                    - Results: Before/after comparison with chemical consistency
                    - Export: Download cleaned data and reports
                
                4. **Export data**: Download imputed_dataX.csv with preserved event data
                
                **Key Features:**
                - Event data preservation
                - WHO & Auckland guideline integration
                - 10% holdout accuracy evaluation
                - Scientific validation
                - Comprehensive reporting
                
                **Enhanced Validation Features:**
                - âœ¨ Pre-imputation holdout dataset creation
                - âœ¨ Accurate quality score calculation
                - âœ¨ Distribution preservation checking
                - âœ¨ Constraint compliance validation
                - âœ¨ Detailed performance metrics (MAE, RMSE, RÂ²)
                - âœ¨ Chemical Balance Enforcement (NOx = NO + NO2, PM2.5 â‰¤ PM10)
                - âœ¨ Photochemical relationships for NOx species
                - âœ¨ Physical models for weather variables
                """)
            
            # Feature highlights
            st.subheader("ðŸŒŸ Key Features")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.markdown("""
                **ðŸ“… Event Preservation**
                - Detects event dates from the 'Air and Traffic Data with Events' module
                - Preserves event-related data
                - Excludes events from training
                - Maintains data integrity
                """)
            
            with col2:
                st.markdown("""
                **ðŸ“‹ Guideline Integration**
                - WHO air quality standards
                - Auckland local guidelines
                - Temperature seasonality
                - Scientific validation
                """)
            
            with col3:
                st.markdown("""
                **ðŸŽ¯ Accuracy Evaluation**
                - 10% holdout validation
                - Quality score metrics
                - Distribution preservation
                - Constraint compliance
                """)


        ##### END #####
        
    else:
        st.error('â›” [ATTENTION] Please confirm your data in the main page to proceed.')
else:
    st.markdown('â›” [ATTENTION] No file found. Please upload and process file/s in the main page to access this module.')

